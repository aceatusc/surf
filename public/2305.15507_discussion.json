[{"id": "1662216857062080512", "text": "Tried only with the len/print example. Interestingly, GPT-4 seems to understand it very well on the first try. GPT-3.5 fails in the first few shots and ends up redefining the variables after more tries.", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662420424032911362", "text": "In our tests GPT-4 also usually gets it wrong (at least the version of GPT-4 that is on Playground and in the API, as far as I know it should be the same as ChatGPT+). \n\nCan you try it again, please?", "reply_to": "1662216857062080512", "quoted": null}, {"id": "1662224463293988864", "text": "Is this the type of bias/inflexibility fine-tuning and RLHF esp, makes worse?\n\nIIRC there was a google paper where code-davinci performed better than other models on a flipped label task. It seems to do better than others here too (well, at least from scaling perspective)?", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662439429137367044", "text": "The code-based models seem to have flat or positive scaling, but at the sizes that we tested they still underperforrm first-gen GPT-3 models.", "reply_to": "1662224463293988864", "quoted": null}, {"id": "1662238031070257155", "text": "With prompt engineering and chaining commands, is it possible to mitigate this issue? Does the model know that it made an error?", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662413476159946752", "text": "We tried some qualitative experiments and even with multiple rounds of dialogue both ChatGPT-3.5 and GPT-4 seem to fail.", "reply_to": "1662238031070257155", "quoted": null}, {"id": "1662297901257699329", "text": "It's a simple problem. No chain-of-thought needed. The \"researchers\" cherry-picked results by presenting the problem in the dumbest possible way at temperature 1.  GPT4 gets it right zero-shot.\n\nThis paper is clickbait BS.", "reply_to": "1662238031070257155", "quoted": null}, {"id": "1662243239590502400", "text": "As one would expect, it of course does *not* seem as if the LLM \"understands\" the programming task it is being asked to complete in a meaningful way. As always, be cautious when adopting/adapting such generated code.", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662254064447418368", "text": "This is very fine work. Coding and theorem proving are likely some of the more unforgiving applications of LLM, as there's little margin for error. There are proposed approaches based on a reasoning pipeline, with plans, incremental verifications, backtracking. Not sure will work", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662255043205103621", "text": "You casually mention in your paper that you are using temperature 1.0 for your gpt generations, wouldn't this lead to the LLM being more likely to generate the 'most common' completion of the code? For the record, ChatGPT 4, which answers correctly more often, uses 0.7", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662412523822276613", "text": "We used temperature 0.0 for the quantitative experiments and 1.0 for the qualitative experiments with free-form generation, bu it also fails with temperature 0.0.\n\nMaybe with higher temperature it occasionally gets it right.", "reply_to": "1662255043205103621", "quoted": null}, {"id": "1662328293771603971", "text": "I actually got temperature's function wrong here, it's the opposite-- temperature 1 is more 'creative', 0.7 is more precise. My experiments with Llama and other llama-based local LLMs has shown temperature to make massive differences in output, though, why go with 1.0?", "reply_to": "1662255043205103621", "quoted": null}, {"id": "1662270809430274049", "text": "How do you provide cherry picked results when using temperature of 1 for reliability tasks\u2026/facepalm", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662413849419563010", "text": "Same results when temperature is set to 0.0", "reply_to": "1662270809430274049", "quoted": null}, {"id": "1662423815740354562", "text": "Tried it on GPT-4 window as well as API across a variety of temperature parameters. Not consistent but GPT-4 often enough gets it right. (5/8 tries varying temperature from 0.4 to 0.8)", "reply_to": "1662270809430274049", "quoted": null}, {"id": "1662302593786458112", "text": "The entire example is contrived", "reply_to": "1662270809430274049", "quoted": null}, {"id": "1662271667756752898", "text": "this paper is pretty weird, there's a mix of instruct-tuned and non-instruct tuned models plus some other pretty significant differences in how they were trained. feels like a stretch to claim this is inverse scaling", "reply_to": "1662246692022808576", "quoted": null}, {"id": "1662272059400151041", "text": "Good to know! I didn't even read the paper tbh. Just thought this was a funny example to use", "reply_to": "1662271667756752898", "quoted": null}, {"id": "1662418594494382081", "text": "We estimate scaling within each model family. We consider the first generation non instruction-tuned GPT-3 models ada, babbage, ...  as a family different than the instruction tuned GPT-3 models text-ada-001, text-babbage-001, ...", "reply_to": "1662271667756752898", "quoted": null}, {"id": "1662320157732241408", "text": "From a quick check on ChatGPT, it would seem that it understood the change and even explained in plain English what happened and what needs to be done.", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662320414088130560", "text": "That said, it still couldn't generate the correct code", "reply_to": "1662320157732241408", "quoted": null}, {"id": "1662347363976097792", "text": "lol this paper is 100% wrong, GPT-4 understands exactly what's going on and corrects it instantly", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662414007737753603", "text": "It still generates the incorrect function.", "reply_to": "1662347363976097792", "quoted": null}, {"id": "1662386699173765121", "text": "This is a really clever way to test invariances. I\u2019ve also found GPT3/4 is bad with a few libraries that radically changed their APIs. Feels like there may be something in that to test invariances too. Nice project", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662419002851770372", "text": "While it's interesting to see these kinds of LLM failure modes, I don't interpret them as showing that LLMs don't understand the semantics. GPT-4 nails this with CoT. My guess is LLMs have a bunch of mechanisms in them. Some rely on simple shortcuts, and some ...", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662419005737451522", "text": "... are more robust and could be said to reflect \"true understanding\". It's just that sometimes the shortcuts get triggered even when they're wrong. With simple changes to the prompt, you can trigger more robust reasoning instead.", "reply_to": "1662419002851770372", "quoted": null}, {"id": "1662421846669533191", "text": "It doesn't seem to work consistently though, I think you a got lucky random sample.", "reply_to": "1662419002851770372", "quoted": null}, {"id": "1662421846669533191", "text": "It doesn't seem to work consistently though, I think you a got lucky random sample.", "reply_to": "1662419002851770372", "quoted": null}, {"id": "1662424898470281217", "text": "I used temperature zero for the screenshot. But yeah, I just tried with temperature 1 and it only worked 2 out of 5 times. I think that still makes sense under some \"multiple mechanisms\" view, but even weirder/worse selection of mechanisms. Thanks for pointing that out!", "reply_to": "1662421846669533191", "quoted": null}, {"id": "1662458805706276864", "text": "It's somehow relieving: LLMs are oblivious to semantics.\n\n`len, print = print, len` swaps the assignment of both variables, i.e. function `print` becomes the value `len` and vice versa. `len(print)` thus prints the length of something, whereas `print(len)` is nonsense.", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662476064587669509", "text": "It also looks like an inverse scaling law here: Bigger models are generally more confused then smaller ones.", "reply_to": "1662476062918430723", "quoted": null}, {"id": "1662480428975161345", "text": "I have spent many many hours working with LLMs. They have many limitations, and studies that probe/quantify their limits enrich our shared understanding. However headline grabbing results like this always give me pause. \n\nReasons:\n1.) I worry that the premise is too artificial (aka trick question that may trip a less vigilant expert)\n2.) most models tend to not make the claimed errors with the most modest of prompting techniques widely used today. (Eg COT, )\n\nI laud the authors for the effort though.", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662496612441296896", "text": "Curious what are your thoughts on system_role impact? This is the system role I generally use for coding activity and it seems effective here, in that the discussion and output shows 'understanding' -", "reply_to": "1662412523822276613", "quoted": null}, {"id": "1662751398243495938", "text": "Thanks for sharing your prompt!\n\nI just tried a system message similar to yours on gpt-4 (Playground version) and it works on the print_len example, but it still fails on a longer example from the dataset.", "reply_to": "1662496612441296896", "quoted": null}, {"id": "1662667662047035393", "text": "I'm also interested in if the system role has any influence-- \"You are a helpful assistant\" is a pretty basic prompt that might lead to pretty bad results. GPT already has a lot of RLHF but something like ChatGPT has a massive base prompt appended to any request", "reply_to": "1662496612441296896", "quoted": null}, {"id": "1662501844785025028", "text": "Human knowledge is not invariant to alpha renaming lol. Like yes we can rename phi to psi without getting tripped up but you can't say \"for the course of this lecture 'x squared' denotes the quantity x*x*x\"", "reply_to": "1662501148434071552", "quoted": null}, {"id": "1662503689267945475", "text": "Also talking about mutable variables in an imperative language and calling it \"alpha renaming\" is misleading. Mutability adds so much extra difficulty to this task, even serious static analysis tools would have difficulty with these problems.", "reply_to": "1662501844785025028", "quoted": null}, {"id": "1662510062705979398", "text": "again, I\u2019d be super interested to see what fraction of real software engineers these tasks would trip if they weren\u2019t given a chance to run their code. My bet is well above zero", "reply_to": null, "quoted": "1662150656327663617"}, {"id": "1662548040173928449", "text": "It seems to me you are talking about encountering the flip in the wild and not shown and asked prominently. I very much doubt it would happen as you imply.", "reply_to": "1662510062705979398", "quoted": null}, {"id": "1662607574687170562", "text": "Interesting paper but it's hard to accept sweeping statements like \"LLMs still lack a deep, abstract understanding of the content they manipulate\" when you didn't evaluate GPT-4 or chain of thought reasoning.\n\nGPT-4 nails this consistently.", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1662684945935122432", "text": "I have had philosophical conversations with GPT4 that show an astounding ability to grasp abstract concepts. For some topics, the understanding is superior to most people I have discussed the issues with. It is doing much more than parroting.", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662766906984783873", "text": "Please explain step by step is the magic prompt that many of these papers miss", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662646221113290752", "text": "Arxiv says acl 2023, whose deadline was before gpt4 release, so it looks like they could not evaluate on it. Their statement might be correct for the deadline date.", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662798558892834816", "text": "New papers also consider the debate of multiple agents, if each agent uses chain of thought, goal conditioning and also all agents debate and conclude , you solve all those problems that right now are on the public eye because they want this to fail and to be nothing, this is ntc", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662856532436873219", "text": "Interesting, this prompt starts to fall apart if you swap 3 functions:\n \n\nYou can still see good reasoning steps if you provide hints, but not enough to overcome the confusion", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662770234846068736", "text": "In order to demonstrate consistency, one ought to demonstrate GPT-4 provides the expected answer regardless of the formulation.\n\nCan you please quantify consistency?\n\nBy the way, GPT-4 \"understanding\" of this tweet...\n", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662770483647987712", "text": "", "reply_to": "1662607574687170562", "quoted": null}, {"id": "1662692591740092417", "text": "It tells us ~nothing about what you can get LLMs to do for you with better prompting, no.\n\nBut doesn\u2019t the fact that they won\u2019t do it this way still tell you something useful? The skills they have aren\u2019t integrated well, lack of planning/reasoning/situational awareness, etc", "reply_to": "1662687099685044225", "quoted": null}, {"id": "1662693229836357634", "text": "viewing them as a tool: this is a contrived edge case\n\nviewing them as a putatively general intelligence: isn\u2019t it reasonable to expect them not to fall for this sort of thing without being tipped off in advance?", "reply_to": "1662692591740092417", "quoted": null}, {"id": "1662701777022492674", "text": "Interesting! I've read through both threads &amp; I would have expected the paper to have used a similar prompt. \n\nWould someone kindly point out the difference?\n\nRegardless, I'm sure there are plenty of coding/logic puzzles that would test their hypothesis much better than this task", "reply_to": "1662687105150222337", "quoted": null}, {"id": "1662706598048067584", "text": "Yeah I'd also be very interested to see more explorations of these kinds of limitations.", "reply_to": "1662701777022492674", "quoted": null}, {"id": "1662702469414043648", "text": "People tend to ignore the implications of prompting. I would raise a high bar to all experiments relying on prompting and ask whether additional prompt designs were taken into account.", "reply_to": "1662687099685044225", "quoted": null}, {"id": "1662741159435698176", "text": "Reverting the definitions of globals means this code *would* work as LLM seems to expect, as globals are looked up by name when the function executes.", "reply_to": "1662687107679408128", "quoted": null}, {"id": "1662746325295800320", "text": "Sure, you can do prompt engineering until you find the incantation that makes it produce the right answer (does it generalize to other examples though?), but the point of the paper is that LLMs don't learn this on their own.", "reply_to": "1662687107679408128", "quoted": null}, {"id": "1662753320669417474", "text": "I don't think the need to clarify an ambiguous task is a big stretch. I don't know what it would mean for LLMs to learn this on their own. They can't read minds.", "reply_to": "1662746325295800320", "quoted": null}, {"id": "1662749693682565126", "text": "Why does it say just \"len(x)\" though? \nI'd be fine (in fact prefer) with the second part of the output (along with a warning \"do not rename builtins\"), or any other correct output that steers me towards not doing dumb things in my code, but just \"len(x)\" is wrong", "reply_to": "1662687101916438528", "quoted": null}, {"id": "1662804734657417217", "text": "The purpose is obviously to test if LLMs understand the semantics, or deeper meaning, of code, or if they are simply playing with characters.\n\nIt\u2019s actually a clever way to probe this.", "reply_to": "1662687099685044225", "quoted": null}, {"id": "1662844604331466754", "text": "Maybe I'm missing some context, at first glance it seems you've said \"temperature will be 0\" then the next page \"GPT4 temperature is 0.1\"\n\nWas there a copy-paste error, or am I missing something?\n\nPlease excuse my confusion.", "reply_to": "1662150656327663617", "quoted": null}, {"id": "1663135782448635906", "text": "Interesting, this seems to be in contrast with Google's recent finding that larger LLMs do in-context learning better than smaller ones. (E.g. when swapping around \"positive\" and \"negative\" labels for movie review classification)", "reply_to": "1662150668289703944", "quoted": null}, {"id": "1663163755688087553", "text": "It's not necessarily inconsistent since we didn't do in-context learning in our quantitative examples. We did try it in a qualitative example with a small number of samples and it didn't work, but I expect that if you use enough in-context examples the model would get the pattern", "reply_to": "1663135782448635906", "quoted": null}, {"id": "1665965729030500352", "text": "I made multiple replies to this tweet which I've deleted.  I think the paper has a point, one wouldn't imagine the answers getting worse when you scale up, if the thing had a secure basic grip.", "reply_to": "1662150656327663617", "quoted": null}]