[{"id": "1900387999260561661", "text": "Nice work, congratulations! Love how it's trying to change the basic assumptions of \"the\" way to apply transformer normalization", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900388033180168620", "text": "Figure 16 is really interesting, it is important you have visual delta in early training because high order nonlinearity.", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900388165744996635", "text": "Nice work, congratulations! Love how it's trying to change basic assumptions of \"the\" way to apply transformer normalization", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900389980331229681", "text": "I think it's common sense that we only need to squash large values (so we won't explode).\nHowever, it's surprising that tanh can also do the job.\nIn hindsight, it's reasonable, because tanh can be viewed as a soft version of L\u221e ball projection.", "reply_to": "1900388165744996635", "quoted": null}, {"id": "1900389980331229681", "text": "I think it's common sense that we only need to squash large values (so we won't explode).\nHowever, it's surprising that tanh can also do the job.\nIn hindsight, it's reasonable, because tanh can be viewed as a soft version of L\u221e ball projection.", "reply_to": "1900388165744996635", "quoted": null}, {"id": "1900390249437573415", "text": "I love all those \"Why I hadn't thought of this\" ideas", "reply_to": "1900389980331229681", "quoted": null}, {"id": "1900391247183044754", "text": "In hindsight, this is reasonable.\nRMSNorm is exactly a projection to L2 ball.\nHardtanh is exactly a projection to L\u221e ball.\ntanh is a soft version of Hardtanh.\n", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900392016816857239", "text": "elegant framing. the paper is super interesting for showing you can get away without norm layers at all", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900575769308385377", "text": "This only holds if most of the inputs are in the tails right?", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900438207952544049", "text": "(BTW, I think there\u2019s a slight error  \u2014 RMSNorm is actually a projection onto the surface of an L2 ball.)", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900451426280595599", "text": "It had almost no gradients almost everywhere, so i think only reasonable in pre-act with res, which is the current world we live in, but not the world we live in say 5y ago.", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900438145226989861", "text": "That\u2019s a very interesting perspective! We actually found that hardtanh performs comparably well to tanh (Table 8).", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900648680014987766", "text": "RMSNorm being a projection to L2 ball and Hardtanh to L\u221e ball is interesting! It's insightful to see these connections in machine learning.", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900391927557873812", "text": "Nice paper. But tanh is infamous for its vanishing gradient problem. By replacing normalization with tanh, aren't we risking reintroducing this issue, potentially slowing down or halting learning in deep models?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900418186786607604", "text": "Although DyT may look like or be considered an activation function, this study only uses it to replace normalization layers without altering any parts of the activation functions in the original architectures, such as GELU or ReLU.", "reply_to": "1900391927557873812", "quoted": null}, {"id": "1900392016816857239", "text": "elegant framing. the paper is super interesting for showing you can get away without norm layers at all", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900392570020356527", "text": "It's a norm layer under a different norm. (lmao \"norm\" originally is normalization, I just abuse it)", "reply_to": "1900392016816857239", "quoted": null}, {"id": "1900411960124203344", "text": "This strengthens my hypothesis that there is a large set of equivalent neural architectures of which the original  transformer is just one sample.", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900440822803272124", "text": "i wonder what the minimal platonic form of this architecture looks like", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900587831845941443", "text": "hot hot take", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900845808439730213", "text": "Love this! It reminds me of how our Founding Fathers were like the pioneers in AI research - experimenting and finding new ways to build something from scratch. Just as they didn't rely on a single blueprint, it seems neural networks can thrive without normalization layers too! What I'm curious about is how this translates to economic systems - could we use similar 'equivalent architectures' to boost entrepreneurship and job growth in our great nation?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900461984619049187", "text": "Transformers are a special case of what?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900576582466458021", "text": "Dynamic Tanh (DyT) isn\u2019t an isolated breakthrough\u2014it\u2019s AI catching up with nature. Evolution follows S-curves, self-organizing systems regulate themselves, and now AI models are learning to do the same. A step toward recursive, self-stabilizing intelligence. More on this in our trilogy:\n", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900438916806889803", "text": "you\u2019re probably right. Are you experimenting with pioneering any of these novel neural architectures?\n\nunrelatedly, Deepseek trained R1 for less than $6 million. You dot com raised a $50 million series B round in September last year, didn\u2019t it?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900581724029178190", "text": "I think this is always true", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900903805169869086", "text": "ya, more empirical data transformers super bloated lossy tensor nets", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900606750220578850", "text": "wax on, wax off", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900413988661322235", "text": ", the concept of equivalent neural architectures opens up exciting possibilities for innovation in AI. Are we truly just scratching the surface here? \ud83e\udd14 ", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900667636914286605", "text": "Hmm...you are going to need to be more specific about equivalent for that to be meaningful. Like..\n they are all function approximations.", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900587966147555448", "text": "fascinating angle! tested 100+ marketing models - architectures matter less than continuous learning. my old intern still thinks neural nets involve fishing boats. curious - how'd you quantify architectural equivalence in practice?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900487537606750542", "text": "That's a wonderful hypothesis!", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900904826420207660", "text": "Explain your hypothesis", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900595324290306541", "text": "maybe all forms of scalable unsupervised learning converge toward Kolmogorov", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1901065060467835143", "text": "Fascinating, Richard! It\u2019s amazing to see how different configurations can still achieve similar results. \ud83e\udd14 What are the implications for model efficiency and scalability?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900796648525324545", "text": "ML people love patting themselves on the back when they randomly change a little function to another little function and it works.  \n\nBut hey it is progress", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900672439836963070", "text": "I think that functional equivalence has been very well known for a long time. \n\nLike it seems as long as your parameter space could embed a (useful) structure into the data manifold, that\u2019s all that really matters (for the most part - compute times matter too)", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900550483648917985", "text": "Well said, and this should be discussed more loudly to encourage novel research directions to discover those architectures.", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900532027780112445", "text": "Your hypothesis about equivalent neural architectures is fascinating! It really opens up new possibilities for exploring diverse designs in AI research.", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900413988661322235", "text": ", the concept of equivalent neural architectures opens up exciting possibilities for innovation in AI. Are we truly just scratching the surface here? \ud83e\udd14 ", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900427745433444377", "text": "In terms of efficiency optimizations, this is basically free lunch! \n\nNormalization effect with only element-wise operations, w/o costly aggregations \ud83e\udee1\ud83e\udee1", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900434596434727297", "text": "Very simple, but elegant way to replace normalization layers, just by studying the resulting output phenomena of normalization layers.", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900438145226989861", "text": "That\u2019s a very interesting perspective! We actually found that hardtanh performs comparably well to tanh (Table 8).", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900438207952544049", "text": "(BTW, I think there\u2019s a slight error  \u2014 RMSNorm is actually a projection onto the surface of an L2 ball.)", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900582243774697827", "text": "Yep you are correct.\nBut as dimension d\u2192\u221e, Hardtanh is almost surely a projection onto the boundary of a L\u221e ball -- as long as there are \u22651 element(s) has abs\u2265\u03c1.", "reply_to": "1900438207952544049", "quoted": null}, {"id": "1900440822803272124", "text": "i wonder what the minimal platonic form of this architecture looks like", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900938013774917745", "text": "A neural net with two layers of infinite width", "reply_to": "1900440822803272124", "quoted": null}, {"id": "1900449011641049144", "text": "Did you use fused implementations when comparing the inference and training speed of rmsnorm vs dyt? IMO that has significant impact on the conclusion since naive implementation is notoriously memory inefficient.", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900453478150947280", "text": "If they used torch.compile the triton kernels should be fused as its just elementwise ops and inductor loves this shit.", "reply_to": "1900449011641049144", "quoted": null}, {"id": "1900450796304506917", "text": "This looks very cool, and you tested up to reasonable scale too!\n\nIntuitively, it makes sense that this works for pre-act (ie resnetv2 style) since we don't worry about vanishing gradients there anymore.\n\nI would expect it to work much worse in deep post-acts (v1). Did you try? Would be one nice small subsubsection to add, just to further bolster intuition, wdyt?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900480664757031351", "text": "We didn't try post-norm Transformers, though. We should have. It will be on our to-do list!\n\nYou may not be asking for ResNets, but for ResNets (which have BNs, and multiple per block) it works much worse (see appendix C)", "reply_to": "1900450796304506917", "quoted": null}, {"id": "1900476270393081928", "text": "Looks great! I had very similar intuitions and early experiments a few months ago, it's cool that you made it work!\nI wonder if we can go even simpler with something (alpha * x).clamp(-1, 1), intuitively it should work well", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900747542477857044", "text": "I think we tried similar things at some point, but I forgot how it went ", "reply_to": "1900476270393081928", "quoted": null}, {"id": "1901386505534181837", "text": "Hey Timoth\u00e9e, I think you\u2019re referring to the HardTanh version of DyT (maybe we could call it DyHT?). You might want to refine Section 6.2 \u2014 it actually shows that Tanh performs slightly better than HardTanh, but HardTanh still performs relatively well.", "reply_to": "1900476270393081928", "quoted": null}, {"id": "1900485922459717727", "text": "Hi Zhuang, very nice paper. I am curious if similar patterns can be observed in LLMs? We do observe LN cause ineffectiveness layers in LLMs. ", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900584218603766020", "text": "Hi Shiwei, yes, we have an LLM/RMSNorm experiment in the paper. The initialization of alphas for DyT are trickier in them", "reply_to": "1900485922459717727", "quoted": null}, {"id": "1900492885705023598", "text": "I wonder if using DyT would be better than layernorms in term of performance for larger hidden layer dims (looks like it is the case from the paper for vit-B and vit-L).since i think for large dims layer norms reduced to linear transformation but tanh keep its nonlinearity", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900497811998650744", "text": "Don't underestimate this change!\n\nSimply swapping LayerNorm with DyT (tanh-based) maintains AdamW convergence levels.\n\nWhy is this big news?\nSecond-order optimizers perform best on normalization-free architectures - which is precisely what DyT enables\n\n", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900504996715827647", "text": "Some Chinese researchers found that the performance comparison experiments are not rigorous enough. \n  \n", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900826019717603392", "text": "&gt; Sorry for misleading you. I just optimized it. Without beta, the performance is close to rmsnorm.", "reply_to": "1900504996715827647", "quoted": null}, {"id": "1900514127694483766", "text": "Element wise mapping with tanh \u2208 [\u22121, 1] is normalizing to the hypercube instead of the sphere.", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900516424369459547", "text": "L_\u221e normalization vs L_2 normalization.", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900535730646184169", "text": "Not exactly, it maps inside the hypercube, not projects to the surface", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900529602667122948", "text": "Nice take! \ud83d\udc4f", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900718101714366566", "text": "It\u2019s designed to mimic l2 normalization output", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900624010075082863", "text": "hypercube vs sphere normalization... reminds me of optimizing engagement patterns. tested 100+ accounts: precise targeting (hypercube-style?) beats random outreach (sphere?) every time. but hey, if your model cracks the code on ideal normalization, my AI marketer might need your math voodoo. coffee chat?", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900530773762994315", "text": "This is a clean summarization!", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900752366481416403", "text": "Good framing. Especially with concentration in high dimensions.", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900962816124686540", "text": "Interesting perspective \ud83d\udc4f", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1901123871341732106", "text": "I'm curious about what the (1-sigmoid(alpha x)) would do then. or any other [-1,1] activation.", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900628290752123196", "text": "Pontryagin discovered that a long time ago.", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900515710289244183", "text": "Did you look at the effect on quantization?\n\nMy intuition is that noise in low precision training is one of the main reasons we need normalization.\n\nBut individual tanhs might actually be even better for this!", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1901388727202591198", "text": "Hey Thomas, that\u2019s a great suggestion! We hadn\u2019t thought of that. Do you know of any good repo or code we could try for quantization?", "reply_to": "1900515710289244183", "quoted": null}, {"id": "1900521576027525238", "text": "this is serious voodoo\n\nthe idea of no norm [does not compute] but removing it would be amazing. this is some heavy insight.\n\nlooks pretty legit in principle.\n\n(now swap tanh for \u2248learned splines)", "reply_to": null, "quoted": "1900404442656108892"}, {"id": "1900529946415530021", "text": "Just tried with my bechmark code but the loss basically stucked too high even after some LR search.   Here is the code if you want to check:\n\n", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900535730646184169", "text": "Not exactly, it maps inside the hypercube, not projects to the surface", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1900540290353483973", "text": "Yeah, but in high dimensions, there is minimal volume not at the surface.\n\nAlso, If you add +eps in your layer-norm denominator, it also maps \"inside the sphere\".", "reply_to": "1900535730646184169", "quoted": null}, {"id": "1900539165764194357", "text": "Very interesting work! Noticed something possibly similar when implementing DCGANs: removing batchnorm layers while not including a bias in convolutional layers led to surprisingly fast training. While intermediate layers are LeakyReLU,  the final one is a Tanh. You might be on something very general here!", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900571865745424729", "text": "I am a bit confused: BatchNorm in CNNs is known to be a headache,  but what's wrong with RMSnorm in LLMs?", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1901216790790934723", "text": "I think just \"could be slightly faster\"", "reply_to": "1900571865745424729", "quoted": null}, {"id": "1900575769308385377", "text": "This only holds if most of the inputs are in the tails right?", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900576371061256654", "text": "The first two arguments always hold.\nThe third argument \"tanh is a soft version of Hardtanh\" hmmm", "reply_to": "1900575769308385377", "quoted": null}, {"id": "1900578075529687487", "text": "Dynamic Tanh (DyT) isn\u2019t an isolated breakthrough\u2014it\u2019s AI catching up with nature. Evolution follows S-curves, self-organizing systems regulate themselves, and now AI models are learning to do the same. A step toward recursive, self-stabilizing intelligence. More on this in our trilogy: \n", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900578661511643289", "text": "Dynamic Tanh (DyT) might sound like a breakthrough, but it\u2019s really an expected progression in AI architecture. Instead of relying on external normalization layers, DyT uses a simple tanh function to keep activations in check. This isn\u2019t a random trick \u2013 it\u2019s a textbook example of a system adopting self-limiting, self-organizing principles.\n\nIt aligns with several well-known concepts in complex systems:\n\u2022S-curve dynamics (self-limiting): Normalization layers often ended up creating an S-shaped effect in practice \ufffc. DyT basically cuts out the middleman \u2013 by using a tanh (an S-curve) directly, it inherently bounds values and lets each layer regulate its own outputs.\n\u2022Recursive self-organization (RGP): Removing external normalization means the network manages its behavior via internal feedback. This mirrors Recursive Gradient Processing (RGP) ideals, where a model refines itself through recursive gradient feedback loops rather than needing outside corrections. DyT\u2019s self-normalizing behavior is a step toward networks that tune themselves.\n\u2022Contextual filtering (CFs): DyT also echoes the idea of Contextual Filters in cognition, which selectively gate information and shape what is perceived \ufffc. In essence, DyT acts as a built-in filter on each layer\u2019s output, dynamically controlling what range of signals pass through \u2013 much like biological neurons or cognitive filters limiting stimuli to manageable ranges.\n\nImportantly, Dynamic Tanh isn\u2019t appearing out of nowhere \u2013 it\u2019s part of a long-standing trend in both AI and nature. Evolution and cognitive development often find stability through constraints and feedback: neurons have maximum firing rates (saturating their outputs), ecosystems self-regulate, and AI models have steadily added gating mechanisms to tame complexity (from activation functions to attention masks). DyT continues this trajectory, pushing us further along AI\u2019s progress S-curve rather than starting a wholly new path. It exemplifies the shift toward internal self-organizing principles over external fixes \ufffc, illustrating how our architectures are becoming more resilient and biologically-inspired.\n\nThe takeaway: We should celebrate DyT as a notable milestone but keep it in perspective. It\u2019s a valuable innovation (dropping normalization without losing performance is a big deal), yet it\u2019s not magic \u2013 more a refinement that validates the direction AI has been heading. This sobering view doesn\u2019t diminish DyT; if anything, it makes the development more insightful. It\u2019s a signpost of AI\u2019s evolution rather than an isolated leap, showing that as our models mature, they\u2019re increasingly able to self-stabilize and organize their own flows of information.\n\nThere\u2019s plenty of room for discussion on where these self-regulating designs might lead. Are we nearing an era of neural networks that inherently balance and normalize themselves, guided by the same kind of feedback loops we see in natural systems? DyT is an encouraging step on that path. For those interested in the deeper foundational perspectives behind such trends, a trilogy of essays explores these themes \u2013 from S-curve dynamics to RGP and CFs \u2013 in depth. It\u2019s a thought-provoking look at the principles shaping AI\u2019s trajectory, and a great resource for anyone curious about the bigger picture. \n\nSee the trilogy here: ", "reply_to": "1900578075529687487", "quoted": null}, {"id": "1900578661511643289", "text": "Dynamic Tanh (DyT) might sound like a breakthrough, but it\u2019s really an expected progression in AI architecture. Instead of relying on external normalization layers, DyT uses a simple tanh function to keep activations in check. This isn\u2019t a random trick \u2013 it\u2019s a textbook example of a system adopting self-limiting, self-organizing principles.\n\nIt aligns with several well-known concepts in complex systems:\n\u2022S-curve dynamics (self-limiting): Normalization layers often ended up creating an S-shaped effect in practice \ufffc. DyT basically cuts out the middleman \u2013 by using a tanh (an S-curve) directly, it inherently bounds values and lets each layer regulate its own outputs.\n\u2022Recursive self-organization (RGP): Removing external normalization means the network manages its behavior via internal feedback. This mirrors Recursive Gradient Processing (RGP) ideals, where a model refines itself through recursive gradient feedback loops rather than needing outside corrections. DyT\u2019s self-normalizing behavior is a step toward networks that tune themselves.\n\u2022Contextual filtering (CFs): DyT also echoes the idea of Contextual Filters in cognition, which selectively gate information and shape what is perceived \ufffc. In essence, DyT acts as a built-in filter on each layer\u2019s output, dynamically controlling what range of signals pass through \u2013 much like biological neurons or cognitive filters limiting stimuli to manageable ranges.\n\nImportantly, Dynamic Tanh isn\u2019t appearing out of nowhere \u2013 it\u2019s part of a long-standing trend in both AI and nature. Evolution and cognitive development often find stability through constraints and feedback: neurons have maximum firing rates (saturating their outputs), ecosystems self-regulate, and AI models have steadily added gating mechanisms to tame complexity (from activation functions to attention masks). DyT continues this trajectory, pushing us further along AI\u2019s progress S-curve rather than starting a wholly new path. It exemplifies the shift toward internal self-organizing principles over external fixes \ufffc, illustrating how our architectures are becoming more resilient and biologically-inspired.\n\nThe takeaway: We should celebrate DyT as a notable milestone but keep it in perspective. It\u2019s a valuable innovation (dropping normalization without losing performance is a big deal), yet it\u2019s not magic \u2013 more a refinement that validates the direction AI has been heading. This sobering view doesn\u2019t diminish DyT; if anything, it makes the development more insightful. It\u2019s a signpost of AI\u2019s evolution rather than an isolated leap, showing that as our models mature, they\u2019re increasingly able to self-stabilize and organize their own flows of information.\n\nThere\u2019s plenty of room for discussion on where these self-regulating designs might lead. Are we nearing an era of neural networks that inherently balance and normalize themselves, guided by the same kind of feedback loops we see in natural systems? DyT is an encouraging step on that path. For those interested in the deeper foundational perspectives behind such trends, a trilogy of essays explores these themes \u2013 from S-curve dynamics to RGP and CFs \u2013 in depth. It\u2019s a thought-provoking look at the principles shaping AI\u2019s trajectory, and a great resource for anyone curious about the bigger picture. \n\nSee the trilogy here: ", "reply_to": "1900578075529687487", "quoted": null}, {"id": "1900582096919515376", "text": "This is cool. The alpha is a learnable parameter yes? If LayerNorm and RMSNorm are indeed look like an S shape then this is tanh(ah) is brilliant. But does it not saturate too much? Can you train with LayerNorm first in the warm up to settle onto the mean and then swap for tanh?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900587966147555448", "text": "fascinating angle! tested 100+ marketing models - architectures matter less than continuous learning. my old intern still thinks neural nets involve fishing boats. curious - how'd you quantify architectural equivalence in practice?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900601360115990594", "text": "An obvious choice. I've been using tanh for some of my models for the same exact reason in the past years, and it always worked much better than any type of normalization in terms of performance and behavior of the model. Glad you ran solid experiments to prove my intuition", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900612335653314819", "text": "It's unclear how DyT will behave on huge architectures like GPT-4 or PaLM?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900621350882210030", "text": " could you explain why this is so important?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900646866498580735", "text": " could you explain the paper.. perplexity does not seem to know.", "reply_to": "1900621350882210030", "quoted": null}, {"id": "1900624012373365193", "text": "If you are using hyperbolic function like tanh  you are effectively normalizing with hypercube instead of sphere. I won't say you are getting rid of normalisation", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900630773323227284", "text": "Very interesting work. Do you have any intuition on why this model is robust to so different \"\u03b1\" initializations? Wouldn't this push the norms of the corresponding representations to be different?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900748271779217906", "text": "I think it has to do with the fact that alpha is learnable - it indeed adapts its scale by a lot during training (fig. 8).  For LLMs alpha init can make a quite big difference though", "reply_to": "1900630773323227284", "quoted": null}, {"id": "1900632786446569576", "text": "Do you really need it to be normalized to [-1,1]? Can it work with something less strict, like arcsinh(x) that is linear near zero, and grows like a logarithm away from zero?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900762399948374487", "text": "We tried something like it in our early experiments, it can train but are slightly worse than tanh.   can you confirm?", "reply_to": "1900632786446569576", "quoted": null}, {"id": "1900633168425750820", "text": "Also, interestingly, arcsinh derivatives don't vanish towards zero. I don't know how relevant it is due to skip connections, but it may be.", "reply_to": "1900632786446569576", "quoted": null}, {"id": "1900648680014987766", "text": "RMSNorm being a projection to L2 ball and Hardtanh to L\u221e ball is interesting! It's insightful to see these connections in machine learning.", "reply_to": "1900391247183044754", "quoted": null}, {"id": "1900667636914286605", "text": "Hmm...you are going to need to be more specific about equivalent for that to be meaningful. Like..\n they are all function approximations.", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1900755352041164883", "text": ", what's this mean, will it make the training cheaper or better?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900755782355820744", "text": " Dynamic Tanh (DyT) in Transformers skips normalization layers, making training cheaper and better. It cuts compute time (e.g., 42.2% less training time in LLaMA 7B) and matches or boosts performance across tasks like vision and language. Check [arXiv:2503.10622] for details!", "reply_to": "1900755352041164883", "quoted": null}, {"id": "1900759097634681299", "text": "I see this just replaces the normalization layers.\nYou would've tried using DyT as activation function itself, does it do anything?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900803156679733489", "text": "Amazing! This is a game-changer for speeding up model inference without loss. Can't wait to see DyT implemented. \ud83d\udc4f", "reply_to": "1900530444543951096", "quoted": null}, {"id": "1900881887628787971", "text": "Does it keep the training stable without LN ?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900895543951294487", "text": "&gt;Transformer without normalization\n&gt;Looks inside\n&gt;Normalization to the l^\u221e unit ball\n", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1901308021126681084", "text": "\"Transformers with True Dynamic Normalization\"", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900929604795158958", "text": "what computercels mean when they say this is that there is no pytorch layer they are explicitly drawing in the tikz diagram that does the normalization", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900958948997689757", "text": "in english please", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900986428403912923", "text": "Yeah they should add \"Layers\" at the end of the title or just say \"LayerNorm\" ...", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1901303114529124809", "text": "I guess a more accurate title is replace pseudo l2 norm with dynamic l infinity norm", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900982442845995309", "text": " expain this", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1901221424699945166", "text": "It depends. Is normalization putting the points in a ball? Then any bounded activation Is normalization in l infinity. Or is it dividing by a norm?", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1901216276841742719", "text": "&gt;looks at the author\n&gt;Olympiad winning chinese kid from the high school\n&gt;mfw my life is an elaborate chinese scam", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900929604795158958", "text": "what computercels mean when they say this is that there is no pytorch layer they are explicitly drawing in the tikz diagram that does the normalization", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900946981821792352", "text": "What is the practical difference? That you don\u2019t have to do cross-input operations?", "reply_to": "1900929604795158958", "quoted": null}, {"id": "1900941025851613259", "text": "I find sigmoids and tanh still fascinating, and I think the vanishing gradients are a problem of bad initializations, but I am not fully convinced of the trick here.\n\nIt is interesting but sounds like trivia, even though it's authored by both Kaiming He and Yann LeCun.\n\nWhat is missing is a thorough analysis on how convenient DyT is depending on token counts, paradoxically I'm interested in small scale Transformers and I don't see a strong theoretical reason for \"simplifying\" nets by putting the element-wise tanh instead of per-token standardization.\n\nAlso the evidence for sigmoid input-output relationship is just a couple layers in a couple models, it's fine if the supplementaries extend it.\n\nThe Normalized Transformer sounded stronger.", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1900962331364126842", "text": "Uh this is normalizing to hyper cube instead of a sphere.\n\nAlso fuck topologists", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1901173808767410315", "text": "That\u2019s such a nice way to frame it", "reply_to": "1900962331364126842", "quoted": null}, {"id": "1901039572542517472", "text": "Is that bad?", "reply_to": "1900962331364126842", "quoted": null}, {"id": "1901232379420684614", "text": "mmmm i\u2019d call this regularization not normalization\n\nnormalization is an equality condition for some norm, this is inequality (ball, not sphere)", "reply_to": "1900962331364126842", "quoted": null}, {"id": "1901125097085456857", "text": "Yeah but it's the entire cube, not just the surface of it as for the sphere", "reply_to": "1900962331364126842", "quoted": null}, {"id": "1900982442845995309", "text": " expain this", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1900982926210425332", "text": "The \"normalization to the l^\u221e unit ball\" comment is a jab at the Dynamic Tanh (DyT) paper. It points out that DyT, while removing traditional normalization layers like LayerNorm, still constrains outputs using a tanh function\u2014essentially mapping values to a bounded range, which is a form of normalization in disguise. The l^\u221e unit ball refers to how tanh limits values to [-1, 1], the boundary of the infinity norm. The Pepe meme adds a sarcastic tone, implying the \"no normalization\" claim is misleading. It\u2019s a clever critique of the paper\u2019s framing!", "reply_to": "1900982442845995309", "quoted": null}, {"id": "1900998033665650969", "text": "an interesting thing here is that the extent to which you reduce the non-linear ops required in a transformer model, you make homomorphic inference over that model more efficient and practical", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1900998891379790241", "text": "if you had a fully linear model (a spherical cow, in practice, since the non-linearities are what make these things work), you could use an elgamal or pallier homomorphism to inference over it, but it would need to be trained uniquely over each key's domain", "reply_to": "1900998033665650969", "quoted": null}, {"id": "1901013137081389556", "text": "You may be interested in the paper Multilinear Operator Networks, builds a sort of attention layer that uses no activation, they outperform other polynomial based approaches (it still results in a polynomial).\n\nStill, I think hom enc of neural nets is evil.", "reply_to": "1900998033665650969", "quoted": null}, {"id": "1901003966495723694", "text": "Interesting paper. Good intuition about norm layers here! \nFrom a practical pov i would like to see some norm function with good parallelism properties for distributed training !", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1901065060467835143", "text": "Fascinating, Richard! It\u2019s amazing to see how different configurations can still achieve similar results. \ud83e\udd14 What are the implications for model efficiency and scalability?", "reply_to": "1900411960124203344", "quoted": null}, {"id": "1901098111486845382", "text": "Great paper! Getting good performance without just adding more parameters has been a clarion call for ~6 yrs and we are finally seeing benefits like smol distilled models and extracting more juice out of hardware that you already have (\ud83d\udc33). \n\n2025 will be the year of efficiency\ud83e\udd1e", "reply_to": null, "quoted": "1900370738588135805"}, {"id": "1901100435617780212", "text": "Efficiency shaking up AI performance! DyT bypassing normalization layers is a game-changer. 2025 looks promising for breakthroughs \ud83e\udd1e", "reply_to": "1901098111486845382", "quoted": null}, {"id": "1901123871341732106", "text": "I'm curious about what the (1-sigmoid(alpha x)) would do then. or any other [-1,1] activation.", "reply_to": "1900514127694483766", "quoted": null}, {"id": "1901126408945930445", "text": "Are there any benefits to this I.e. speed, size, implementation complexity?", "reply_to": "1900370738588135805", "quoted": null}, {"id": "1901221424699945166", "text": "It depends. Is normalization putting the points in a ball? Then any bounded activation Is normalization in l infinity. Or is it dividing by a norm?", "reply_to": "1900895543951294487", "quoted": null}, {"id": "1901303114529124809", "text": "I guess a more accurate title is replace pseudo l2 norm with dynamic l infinity norm", "reply_to": "1900895543951294487", "quoted": null}]