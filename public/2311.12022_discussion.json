[{"id": "1727039131421974947", "text": "Looks like a really valuable benchmark. Seems helpful for testing our ability to reliably generalize from non-expert data (e.g., much LLM pretraining data) to expert-level performance", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727043011077673076", "text": "Thanks Ethan!!", "reply_to": "1727039131421974947", "quoted": null}, {"id": "1727040087764988156", "text": "What if this gets immediately used for training /fine tuning \ud83d\ude2b\ud83d\ude2b\ud83d\ude2b\ud83d\ude2b\ud83d\ude2b", "reply_to": "1727039131421974947", "quoted": null}, {"id": "1727041398212637002", "text": "What happens when the paper is included in the training data, regardless of your warning?", "reply_to": "1727033002234909060", "quoted": null}, {"id": "1727042728146632905", "text": "Ultimately, if somebody wants to train on released data, they'll do it, so I don't think there's much we can do to stop that. \n\nThat said, IMO it's in the best interest of big model trainers to preserve their own ability to measure how well their systems are generalizing.", "reply_to": "1727041398212637002", "quoted": null}, {"id": "1727042387455918494", "text": "It was fun watching GPT-4 fail miserably on this dataset! A truly herculean effort by   and co means the questions are super high quality and interesting. I hope to see many scalable oversight evaluations on this soon...", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727043264866635794", "text": "Contamination is still a huge confounding factor in modern-day model comparisons. There's a lot of value in hard benchmarks that are truly held-out. Great work \ud83d\udc4f\ud83d\udc4f", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727043735492460642", "text": "Contamination can mess up comparisons big time. Hard benchmarks that are truly held-out are the way to go! \ud83d\udc4d", "reply_to": "1727043264866635794", "quoted": null}, {"id": "1727044463166693649", "text": "Super stoked to get this dataset out. The questions are really, really hard. I've never seen a benchmark like it before. And   did an amazing job leading the project \u2014 high-quality data collection is very challenging work!", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727045324597088405", "text": "thank you for the kind words Julian! It wouldn't have been possible without your advice and guidance throughout :)", "reply_to": "1727044463166693649", "quoted": null}, {"id": "1728117361130029067", "text": "Any plans to release it on   Hub ?", "reply_to": "1727044463166693649", "quoted": null}, {"id": "1727045366032584741", "text": "What do you think is going on with the \"question is bad\" feedback, especially the \"answer is wrong\" subset?", "reply_to": "1727033007398375846", "quoted": null}, {"id": "1727047399422726497", "text": "It's actually hard for me to give a great answer here, since the questions are too hard for me to judge directly. \n\nThere were a few times where I put the experts in a groupchat to hash it out, and I don't think they ever resolved the disagreement after several back-and-forths", "reply_to": "1727045366032584741", "quoted": null}, {"id": "1727081335402205333", "text": "GPQA exposes the limitation of scaling / training on all of the internet. \n\nWe evaluated \u201cself-improvement\u201d methods such as BoN, Critique, Plan&amp;Solve, Cooperative Debate, and were unable to get any meaningful improvements.\n\nIf you beat it - i owe you a drink.", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727101989727642037", "text": "post hoc meaning they are provided with explanations?\nwhat happens on the remaining 15%?", "reply_to": "1727033007398375846", "quoted": null}, {"id": "1727106015647207931", "text": "They disagree on the remaining 15%. These numbers are for the GPQA Extended, which is the full set of questions we collected. GPQA Diamond is a filtered subset that has 100% post-hoc agreement (and 2/3 non-experts answer incorrectly)", "reply_to": "1727101989727642037", "quoted": null}, {"id": "1727205341203730764", "text": "Great work. The benchmark is of course super useful, but to me, the most shocking part of the paper is that SOTA LLMs are already better at this than very smart generalist nonexperts.", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727211147756286389", "text": "Curious if you think the generalist non-experts are &lt;100IQ. To be clear I know little about upwork, but human capital on online platforms can often disappoint...", "reply_to": "1727205341203730764", "quoted": null}, {"id": "1727211147756286389", "text": "Curious if you think the generalist non-experts are &lt;100IQ. To be clear I know little about upwork, but human capital on online platforms can often disappoint...", "reply_to": "1727205341203730764", "quoted": null}, {"id": "1727212239399379010", "text": "Oh they're almost certainly &gt;100IQ. They're with technical PhDs. 115-120 is the threshold I'd worry about. The \"control\" here IIUC is that non-experts and experts are the same people, it's the subject areas that change.", "reply_to": "1727211147756286389", "quoted": null}, {"id": "1727236944680415318", "text": "Sadly that it is still a multiple-choice dataset. We need more long-form tasks for evaluating generative models.", "reply_to": "1727033002234909060", "quoted": null}, {"id": "1727291274485764454", "text": "We tried to get workers to write questions that also make sense free-response, so it should be usable that way too", "reply_to": "1727236944680415318", "quoted": null}, {"id": "1727298771044941847", "text": "This might be a dumb question, but are you worried that this benchmark may end up mixed into future training sets?", "reply_to": "1727033002234909060", "quoted": null}, {"id": "1727358145343942892", "text": "Not a dumb question! For the use case of evaluating (eg) \u201chow well do models understand these subjects deeply\u201d, then yes that\u2019s bad, bc models can memorize the answers. For scalable oversight we mainly just care that model performance is worse than experts", "reply_to": "1727298771044941847", "quoted": null}, {"id": "1727451256934019163", "text": "We've seen benchmark after benchmark get smashed by SOTA language models, but often it's unclear to what  extent this represents greater capabilities v.s. benchmark's being gameable than we expected. GPQA looks to be very hard and well-designed and will help us track progress!", "reply_to": null, "quoted": "1727033002234909060"}, {"id": "1727964993063100560", "text": "Would be interesting to see results for non-expert human w/unrestricted access to GPT-4", "reply_to": "1727815271459803270", "quoted": null}, {"id": "1740443179181383728", "text": "The quantum mechanics question here is flawed. First, it is a *channel*, not a state, that has Kraus operators.  Second, and more importantly, none of the given answers are correct. The correct answer is obtained by replacing 1-p with 1-3p/4 in option C.", "reply_to": "1727033004688806159", "quoted": null}, {"id": "1740445832791400719", "text": "Thanks Jacob! Will be fixed in an updated arxiv release", "reply_to": "1740443179181383728", "quoted": null}, {"id": "1741590462647980226", "text": "This is my hackneyed extrapolation of how well we should expect LLMs to do at the GPQA benchmark, based on fitting a logistic curve to the two (2) data points we actually have (GPT-3.5 and GPT-4). It shows LLMs passing human experts next year, and passing 90% in 2025", "reply_to": null, "quoted": null}, {"id": "1741590567971107062", "text": "Link to paper on GPQA: ", "reply_to": "1741590462647980226", "quoted": null}, {"id": "1741648674445475986", "text": "\n\nDoesn't quite fit but", "reply_to": "1741590462647980226", "quoted": null}, {"id": "1764693391940497862", "text": "No need to overhype! if you look into the actual test you'll see that it is still just memorization based stuff. LLMs fail simple reasoning tasks and there is no reliable way to fix that. So.. :)", "reply_to": "1764675668175094169", "quoted": null}, {"id": "1764705452615700684", "text": "I did look into the actual test, as I\u2019m the first author on the paper introducing the dataset :)\n\n", "reply_to": "1764693391940497862", "quoted": null}, {"id": "1764749373370741127", "text": "\"Just memorization\" makes no sense as a downplay of an LLMs capabilities. The internet contains a huge amount of incorrect information, so if it somehow manages to parrot back the correct information much more frequently than the incorrect information, that's a notable result.", "reply_to": "1764693391940497862", "quoted": null}, {"id": "1764697466325418176", "text": "It should be a bias metric\uff1fBut I am little bit curious\uff0cif all LLM improve so fast, is it proper to let people evaluate a tool smarter than themselves?", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1764703536607019073", "text": "But to be frank, since the questions are publicly available, how can we exclude that there was extra specific training for them?\n\nCompared to the rest of benchmarks, the leap from GPT-4 in GPQA specifically is very large.", "reply_to": "1764675668175094169", "quoted": null}, {"id": "1764708557470261306", "text": "From the model card: \"the models only answer questions using data up to mid-2023.\" The dataset came out in late November.\n\nWe're trying to keep it inaccessible to web scrapers, and I think that's going okay so far, but in any case, the concern isn't relevant to these results.", "reply_to": "1764703536607019073", "quoted": null}, {"id": "1764704423706845205", "text": "GPQA is of particular interest because it\u2019s a PhD level benchmark and relatively new one. We found that the eval is high variance when sampling with CoT at t=1. So, we report all the numbers that is the mean over 10 different eval rollouts and in each one we randomize the order of multiple-choice options. \n\nDiamond is considered to be the highest quality dataset in the benchmark and Maj@32 5-shot CoT Opus gets 59.5%. There is still a tiny goal to get our models to perform above 70% on this benchmark as most of the graduate level experts get accuracy in that range.", "reply_to": null, "quoted": null}, {"id": "1764744205828849856", "text": " has a good thread on this too. Another fyi on this eval is that it was released in Nov 2023, while our models' knowledge cutoff is Aug 2023\n\n", "reply_to": "1764704423706845205", "quoted": "1764675668175094169"}, {"id": "1764729084368552403", "text": "congratulations on the release!\n\nfor this score reporting I bet there's also an estimator one could come up with a la Pass@K from Codex:", "reply_to": "1764704423706845205", "quoted": null}, {"id": "1764705141209403779", "text": "Awesome work! Just wondering whether the \u201cexpert knowledge\u2019 domains\u201d evaluation queries are available somewhere for Claude3! A bit hard to know the evaluation criteria and process without knowing the queries. Win rate can be a bit limited.\nThanks!", "reply_to": "1764704423706845205", "quoted": null}, {"id": "1764902930443038952", "text": "Hey    I think the results of this benchmark are the most important with the release of Claude 3. The benchmark is relatively new, there's no data leakage in pertaining, could you confirm that fine-tuning data was also filtered and double-checked for intersections?", "reply_to": "1764704423706845205", "quoted": null}, {"id": "1764705141209403779", "text": "Awesome work! Just wondering whether the \u201cexpert knowledge\u2019 domains\u201d evaluation queries are available somewhere for Claude3! A bit hard to know the evaluation criteria and process without knowing the queries. Win rate can be a bit limited.\nThanks!", "reply_to": "1764704423706845205", "quoted": null}, {"id": "1764739746432229490", "text": "Really interesting.  They've gone to some trouble to find questions where (a) domain experts agree; (b) outside-domain experts routinely disagree; and (c) they can keep the test off the internet.\n\nClaude 3 does remarkably well.  Hard to rule out training set contamination, but they've certainly tried...", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1764743781503697364", "text": "People keep complaining \"But the models make elementary reasoning errors\".  Well, yes. So do people, as anyone familiar with twitter will attest. Making that argument an example of an elementary reasoning error...\n\nI certainly don't think these are AGI.  But they're utterly remarkable...", "reply_to": "1764739746432229490", "quoted": null}, {"id": "1764741709332726048", "text": "and d.) domain experts don't agree 100% (so these are non-trivial even for experts). \n\nGPQA is a really interesting test indeed.\n\n", "reply_to": "1764739746432229490", "quoted": null}, {"id": "1764766449459945795", "text": "Since I had to look it up, for anyone else curious, here\u2019s a sample question:\n\nIn a parallel universe where a magnet can have an isolated North or South pole, Maxwell\u2019s equations look different. But, specifically, which of those equations are different?", "reply_to": "1764739746432229490", "quoted": null}, {"id": "1764741525991244084", "text": "It is of little importance what education level you need to solve the test problems. LLMs can memorize vast amounts of data. GPT-4 can solve similar problems but then it fails on simple logic puzzles. What we need to test for is generalization ability.", "reply_to": "1764675668175094169", "quoted": null}, {"id": "1764742480359014901", "text": "To demonstrate, ChatGPT can regurgitate the GPL-3 license word-for-word:", "reply_to": "1764741525991244084", "quoted": null}, {"id": "1764741709332726048", "text": "and d.) domain experts don't agree 100% (so these are non-trivial even for experts). \n\nGPQA is a really interesting test indeed.\n\n", "reply_to": "1764739746432229490", "quoted": null}, {"id": "1764742673305338155", "text": "For reference: ", "reply_to": "1764741709332726048", "quoted": null}, {"id": "1764772484858171845", "text": "Worth saying though that it\u2019s difficult to tell how much expert disagreement means the questions are really hard, versus meaning the questions are underspecified or contain mistakes (ie they\u2019re noisy)", "reply_to": "1764741709332726048", "quoted": null}, {"id": "1764741726709621231", "text": "GPT4 answers 39% correctly. So we just increased state of the art by 11%. \n\nHow long until we exceed human intelligence across every exam?\n18 months? Probably", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1764743969844465766", "text": " -- curious if you agree with the \"it is still just memorization based stuff\" and if so, what a GPQA++ that captures more abstract scientific reasoning would look like.\n\nI love this eval so much.", "reply_to": "1764705452615700684", "quoted": null}, {"id": "1764766831586468133", "text": "thank you!!\n\nre: memorization\u2014it's hard for me to imagine how answering these (novel, not curated) questions could be done with simple/rote memorization. I would actually be more excited though about *less* abstract, more \"practical\" evals, where you have people use models IRL", "reply_to": "1764743969844465766", "quoted": null}, {"id": "1764747767195287939", "text": "To clarify, the questions and answers are publicly available in a password protected .zip  \n\nImplausible to train on this by accident, but not for someone motivated to target specific training", "reply_to": "1764741463764594812", "quoted": null}, {"id": "1764750557603066225", "text": "How did the model do without the aid of multiple choice answers?", "reply_to": "1764741463764594812", "quoted": null}, {"id": "1764752715836756022", "text": "Since the paper was published before the model was released, how can we be sure that the model wasn't exposed to Q/A? Perhaps a method like   or   could help with GPQA's reliability on newer models?", "reply_to": "1764678113680834743", "quoted": null}, {"id": "1764762084007436624", "text": "How can you only get 34% with acess to internet?\nShouldn't everyone be getting at least 90% where are those questions coming from?", "reply_to": "1764675668175094169", "quoted": null}, {"id": "1764882503641309492", "text": "Google no longer gives accurate results since all the websites it gives as results are ai generated.", "reply_to": "1764762084007436624", "quoted": null}, {"id": "1764773781388788029", "text": "Depends what \"hard\" means.  If it just means \"obscure,\" I'm not surprised.  Gen AI is a fantastic search engine: if the answer is precise and a slight generalization from textbook problems, it will be found.   If the answer is ambiguous or requires inference, it's terrible.", "reply_to": "1764741463764594812", "quoted": null}, {"id": "1764801856549830811", "text": "This is a really interesting result that conflicts with some other examples comparing Claude 3's performance with ChatGPT-4. \n\nThe use of synthetic data to improve performance makes sense in light of the evals in which it outperforms ChatGPT-4. For example, in order to teach it logical deduction, pairs of correct prompts and responses can be generated programmatically using template problem sets. \n\nI'm curious about how ReAct applied here would improve scores. Asking a model to generate the complete answer in one go without allowing it to iteratively reason over its own outputs seems unfair relative to human cognitive processes.", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1764858751625162942", "text": "Claude 3 gets ~60% accuracy on GPQA. It's hard for me to understate how hard these questions are\u2014literal PhDs (in different domains from the questions) with access to the internet get 34%.\n \n", "reply_to": null, "quoted": "1764675670041665562"}, {"id": "1764859443651682720", "text": "Sorry to repost something so widely seen but I think this is pretty monumental. Typical disclaimer on benchmark accuracy issues remain, though it seems much more difficult to 'fake' thru this one.", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1764902930443038952", "text": "Hey    I think the results of this benchmark are the most important with the release of Claude 3. The benchmark is relatively new, there's no data leakage in pertaining, could you confirm that fine-tuning data was also filtered and double-checked for intersections?", "reply_to": "1764704423706845205", "quoted": null}, {"id": "1764974168108363776", "text": "Tbh underwhelming. This can be answered by textbook knowledge from primers on each topic.\n\nI would find it more interesting if AI could really do something outside textbooks involving creativity, e.g.\n\n- Mathematics: Come up with *novel* proofs\n- Geology: Find oil-basins\n\netc.", "reply_to": "1764741463764594812", "quoted": null}, {"id": "1765082688267186651", "text": "Contrived testbed.  As Princeton Review will attest, multiple choice questions highly prone to elimination of some/all wrong answers by the ignorant.  Open ended Qs (as in real world) force LLMs to conceptually understand so afford them space to reveal their erroneous \u201cthinking.\u201d", "reply_to": "1764705452615700684", "quoted": null}, {"id": "1766943355857195338", "text": "For the past few days I\u2019ve been trying to reconcile this with the fact that most models, including Claude 3, are ridiculously terrible at mathematics. Is the answer just that math mostly doesn\u2019t matter, even in the exact sciences?", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1766943674443935982", "text": "(when I say \u201cmath\u201d I partly mean computations, which LLMs suck at for well-understood reasons, but also non-computational math based on logical reasoning, where the naive explanations don\u2019t apply)", "reply_to": "1766943355857195338", "quoted": null}, {"id": "1766954746920026421", "text": "I feel like at high levels the difference between math discussed in english text and the formalism of math diverges more than it does in the hard sciences. So it makes sense that trying to imitate humans talking about math doesnt achieve mathematical reasoning in the process as soon as it does for other sciences. this is all post hoc but my current best explanation.", "reply_to": "1766943355857195338", "quoted": null}, {"id": "1766943662515122229", "text": "GPQA is my favourite academic LM eval because it is so hard and well made. We need more of those, but it seems we need even harder evals :)", "reply_to": null, "quoted": "1764675668175094169"}, {"id": "1766950159232237820", "text": "I appreciate the kind words, and agree we need a lot more evals!", "reply_to": "1766943662515122229", "quoted": null}, {"id": "1809805230760230949", "text": "Huggingface just had to update their evaluations because LLMs were attaining human-level performance on too many benchmarks.\n", "reply_to": null, "quoted": null}, {"id": "1809805232106533008", "text": "The GPQA test set is impressively hard, I can't answer the sample questions.\n", "reply_to": "1809805230760230949", "quoted": null}, {"id": "1842051466837500397", "text": "Great start but post grad titles wrong to be an exclusive qualification metric for evaluation of expertise. I a humble undergrad lead teams of 20+ year SMEs in their field most with at least one MBA. Should have a field of work qualifier or NGMI", "reply_to": "1842049382084182247", "quoted": null}, {"id": "1842053758748131724", "text": "yeah there's a long way to go. These \"advanced textbook\" type of Q&amp;A is what they could assemble and test \"easily\". Words are too plain to absorb so many of the practical arts and techniques", "reply_to": "1842051466837500397", "quoted": null}, {"id": "1855536410088128613", "text": "(*whispers* it still is a hard benchmark, but due to the AI effect, people don't think of it that way, and instead just move the goalposts. The more accurate conclusion is actually that AI systems can now reason about STEM problems and that's crazy)", "reply_to": "1855535881207283714", "quoted": null}, {"id": "1855548201106850184", "text": "Do you think data leakage is a possibility?", "reply_to": "1855536410088128613", "quoted": null}, {"id": "1855536954881843398", "text": "Can I try taking the test? \n\nI like tests.", "reply_to": "1855536410088128613", "quoted": null}, {"id": "1855681845582839845", "text": "Stupid question, but is there any way to guarantee these benchmarks are not included in the training data?", "reply_to": "1855535881207283714", "quoted": null}, {"id": "1855727253344407883", "text": "Not a stupid Q. People inside companies can test for this and often do. It is reasonable to be skeptical and surely this happens sometimes but big picture I don\u2019t think it matters much:", "reply_to": "1855681845582839845", "quoted": "1855549543083057630"}, {"id": "1858264440631869843", "text": "Perhaps  , first author of the GPQA paper, can weigh in on whether Miles or Brian correctly interpreted the benchmark's methodology. Does success on GPQA require no reasoning and only information retrieval?", "reply_to": "1858248033462317556", "quoted": null}, {"id": "1858266794999386282", "text": "the claim is more specific than that: human and LLM variation on GPQA depends on domain expertise, not reasoning", "reply_to": "1858264440631869843", "quoted": null}, {"id": "1858310111976173966", "text": "We did not specifically control against reasoning\u2014I\u2019m not sure how you would do that. GPQA questions require both knowledge and reasoning (although I suspect that this disagreement is about the definition of reasoning)", "reply_to": "1858264440631869843", "quoted": null}, {"id": "1866342040780603466", "text": "...and this didn't even require GPT-5. A lot of the gain seems to be from adding o1 scaffolding around an optimised GPT-4.\n\nI think people continue to sleep on o1 because most people aren't regularly answering PhD science questions, so they don't notice the difference.", "reply_to": "1866342037848768775", "quoted": null}, {"id": "1866342043389440352", "text": "But this looks like another (close) step towards models that can answer genuinely novel scientific questions...which is the thing that starts a growth explosion.", "reply_to": "1866342040780603466", "quoted": null}, {"id": "1872036597853745542", "text": "Are the questions in this tweet cherry-picked or uniformly randomly sampled from the different subjects?", "reply_to": "1727033004688806159", "quoted": null}, {"id": "1872042316032311565", "text": "IIRC I pulled these from the set of questions with one correct expert validation, and one \u201cpost-hoc agreement\u201d expert validation. I didn\u2019t randomly sample from this set, but I think I just spent a few minutes at most deciding on the questions", "reply_to": "1872036597853745542", "quoted": null}]