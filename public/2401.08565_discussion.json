[{"id": "1747628376389550089", "text": "Is this just boosting? Or is there something else involved?", "reply_to": "1747524840586723646", "quoted": null}, {"id": "1748022920008364201", "text": "Are people already using OpenAI output logits when using GPT-4 to create synthetic datasets? Seems like it would give you much more information, similar to this proxy-tuning approach.\n\nI wonder how long before OpenAI stops supporting logit outputs :(", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748025996933050865", "text": "I thought the OpenAI API only outputs top-k logits, not all logits for the whole vocabulary. Pls correct me if I'm wrong.", "reply_to": "1748022920008364201", "quoted": null}, {"id": "1748025357955961022", "text": "They would probably only need to stop supporting logit outputs once their edge/revenue starts getting eaten by other models.... which might be a while haha\n\nI guess they could start only enabling it for older models but people will essentially mine the logits from outputs anyway", "reply_to": "1748022920008364201", "quoted": null}, {"id": "1748023768037933546", "text": "Question: Is the intended use case deployment on edge devices ? Cause one of the reason to use lora is GPU efficiency and this method looks like it needs 3x GPU usage.", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748024634241348048", "text": "I think the intended use-case is probably more like R&amp;D efficiency: develop new methods, use them on smaller models to save costs, then use these to boost your bigger base models", "reply_to": "1748023768037933546", "quoted": null}, {"id": "1748024347183480838", "text": "I love how the community is (slowly) accepting that we can and should raise the level of abstraction on ML terms.\n\nOptimizing without modifying the weights? Yes!\n\nWe teach undergrads the difference between problems &amp; algorithms. You care about the *spec* not how it's implemented!", "reply_to": null, "quoted": "1748021765790376385"}, {"id": "1748030551787717076", "text": "Black-boxification of LLMs haha", "reply_to": "1748024347183480838", "quoted": null}, {"id": "1748024605585871153", "text": "IIUC, you modify the output logits with the formula you showed M1*(x) = M1(x) + [M3(x) - M2(x)]. Then, in order to generate text, you convert the logits to probabilities, and use something like nucleus sampling or top-k sampling to the new probabilities. Right?", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748024919244382481", "text": "Yes, that's correct. This detail is not shown in the figure, but yes that's what you would do.", "reply_to": "1748024605585871153", "quoted": null}, {"id": "1748025974669901910", "text": "Impressive benchmark results.\n\nAs the paper says \"that tokenizers are often open-source, even for closed-source models like GPT-4 (i.e. tiktoken), making it feasible to steer these models with small, open-source models. When vocabularies do not match, techniques like \"Twist Decoding: Diverse Generators Guide Each Other\" Kasai et al. (2022) could be applied.\"", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748027644677914844", "text": "This really reminds me of many first-order approximations that you find in physics that so often work nearly as well as the full computation", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748028787995086906", "text": "Seems like base models are getting \u201cgood enough;\u201d yet still so many opportunities to iterate downstream (on new approaches to finetuning and prompt evolution) and upstream (better data curation)", "reply_to": null, "quoted": "1748021765790376385"}, {"id": "1748030281058288020", "text": "Why does this work? \ud83e\udd14", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748031001618464839", "text": "I suspect that's because the Delta between a 7B Base and 7B Chat is similar to the Delta between 70B Base and 70B Chat, which is a positively surprising observation.", "reply_to": "1748030281058288020", "quoted": null}, {"id": "1748030792624935034", "text": "waittt this is not original, this was done back in october ", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748075477703479741", "text": "Whoa, thanks for sharing, this indeed looks like the same thing: \"we can emulate the result of pre-training at 70B scale and fine-tuning at 7B scale by performing the log probability algebra Llama-2-base 70B + (Llama-2-chat 7B - Llama-2-base 7B), where the first term is the base log probabilities and the term in parentheses is the behavioral delta\"\n\nThe only very tiny difference is that proxy-tuning applies this to the logits and then computes the probas whereas this method applies it to the probas (or log probas) directly.", "reply_to": "1748030792624935034", "quoted": null}, {"id": "1748605662186426449", "text": "we discuss the relationship to EFT (concurrent but independent work) in our related works \u2014 our paper goes further by quantifying effectiveness on benchmarks &amp; experimenting w/ domain + task tuning. thanks   for also noting what both papers share in common w/ DExperts!", "reply_to": "1748030792624935034", "quoted": null}, {"id": "1748031547536544076", "text": "Wait, then what happen if you apply it on 70B chat?\ud83e\udd14\n\nSounds like free improvement for any model you want", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748074367114793308", "text": "Good question actually. Theoretically, this should not work super well because you are probably overcompensating, but it would be an interesting data point to add.", "reply_to": "1748031547536544076", "quoted": null}, {"id": "1748031659167928553", "text": "I\u2019m curious if this could be used to combat the backdoors examined in Anthropic\u2019s Sleeper Agents paper\u2026 Would this fine-tuning be enough to overcome such triggers? Would it require excessively heavy handed tuning to do so?", "reply_to": null, "quoted": "1748021765790376385"}, {"id": "1748039405489516757", "text": "Could that be used for weak-to-strong generalization as a smaller model can make sure the large model behaves in a desired way?", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748041476880400848", "text": "Wonder if proxy-tuning can replicate the performance gains of instruction tuned models. \ud83e\udd14", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748068042615976291", "text": "Can this be applied to other ML models that have decoders e.g. Whisper?", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748085650664910906", "text": "I would wonder if the same works inversely. Taking the difference between 70b chat and 70b base, then applying it to a 7b model. \n\nCould be a cheap and dirty way to improve tons of smaller models.", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1748169662196551883", "text": "\u2026 Sad how the authors cite this given that most of their novelty is contained in this work (EMNLP '13). We worked on the same idea: substracting the base model didn't help, maybe the authors can put this ablation in the paper.", "reply_to": "1748077043827318913", "quoted": null}, {"id": "1748307022565499298", "text": "11 years ago? Please cite precise reference. There was a surprisingly similar paper from 1 year ago indeed, but 11??", "reply_to": "1748169662196551883", "quoted": null}, {"id": "1748202358595817950", "text": "Yes, this does seem basically identical to our Emulator for Fine-Tuning LLMs ( ). Thanks for noticing,  ! But, as   &amp; Martin note (2nd ed., \u00a71.6.7,  ), following Merton, science has a lot of multiple discoveries.", "reply_to": "1748169662196551883", "quoted": null}, {"id": "1748280216894550193", "text": "Always like these scale-difference based approaches - seems to work &amp; be good for efficiency &amp; to avoid catastrophic forgetting. Right that vocab is an issue at logit level - would byte-level encodings help? But how to retrofit to fixed vocab models (extra transformation layers?)", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1749342873039110303", "text": "Thanks for this post! I have one question, is there any backpropagation happening? I see that the whole point of this is not change the parameters of 70B model. So anytime we want to do inference on the 70B model, we need both the small tuned and untuned ones?", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1749409290077491209", "text": "That\u2019s correct, there\u2019s no backprop happening. Any yeah, you\u2019d need to keep the larger models around.", "reply_to": "1749342873039110303", "quoted": null}, {"id": "1749354006340853805", "text": "If my understanding is correct, do we need two extra models (7B-Chat and 7B) at each decoding step to get the prob to offset.  This would be a big overhead for recomputing the KV-cache of 7B models?", "reply_to": "1748021765790376385", "quoted": null}, {"id": "1750608839182028920", "text": "Looks interesting, will definitely try it out! Does it also work for chain of thought prompting where the 3 models can potentially have different outputs and hence, arithmetic operations on logits might not be valid?", "reply_to": "1750549967599804916", "quoted": null}, {"id": "1752368665704411276", "text": "So step 3 is basically vector subtraction between the logits of the finetuned small model and the logits of the same small model before fineuning?\n\nWe then do vector addition between this delta and the logits of the big model.\n\nAnd this delta is different for every input?", "reply_to": "1752349456421261335", "quoted": null}, {"id": "1752370095198711887", "text": "Yes, to use the code example from my article above.", "reply_to": "1752368665704411276", "quoted": null}, {"id": "1752536610032492762", "text": "Very interesting! Thanks for sharing.\n\nConceptually speaking, is the case that in this approach we are combining three models (Target + small tuned - small base) to produce one final model?\n\nIf it is the case, can we look at these three models as Experts and we are explicitly combining them in parametric way (Target + small tuned - small base)?\n\nFor instance, I am curious to know if one can fine tune the way these models are combines like Target + lambda_1 *small tuned - lambda_2*small base? or more generally some very lightweight function F(Target, small tuned, small base)?\n\nThanks!", "reply_to": "1752349456421261335", "quoted": null}, {"id": "1753173434366861712", "text": "Is the net benefit that less $$$ has to be spent to get custom 70B models?\n\n1. Feels like by training the \"small tuned LLM\" less hardware is needed for the fine tuning. \n2. Feels like for the proxy inference, you then run 3 models but if the models are quantized then you still are using less hardware. \n\nIs the above intuition correct?", "reply_to": "1752349456421261335", "quoted": null}, {"id": "1784865923309977916", "text": "We tried it. Performance really deteriorated. The idea is good but on serious tasks it doesn\u2019t work that well from our experience \ud83d\ude33", "reply_to": "1784559710978404861", "quoted": null}, {"id": "1786852045577994702", "text": "Most interesting thing is when fine tuned on TruthfulQA, the performance for proxy tuned model is better compared to SFT model, as that's better able to retain factual knowledge from its pre-training stage.", "reply_to": null, "quoted": "1786518726537318734"}]