[{"id": "1772805343577276784", "text": "Why doesn\u2019t hyperparameter tuning catch these kinds of things?", "reply_to": "1772803686965694684", "quoted": null}, {"id": "1772813812464513433", "text": "They were pruning layers after pre-training with optional \"healing\" (fine-tuning after pruning) afterwards.\n\nThe fact that the final model is reachable theoretically doesn't mean pre-training with the same shallow model architecture from the beginning will ever get you there...", "reply_to": "1772805343577276784", "quoted": null}, {"id": "1772820543055573360", "text": "It doesn't seem minimal to me.. unlike LASER they are not retaining performance with pruning.", "reply_to": "1772803686965694684", "quoted": null}, {"id": "1773046609267347867", "text": "interesting paper - hopefully the first step to finding ways of reducing the number of layers and parameters in LLMs while preserving good performance", "reply_to": null, "quoted": "1772803686965694684"}, {"id": "1773071929231933544", "text": "Given how much data these LLMs see during training, is the lack of performance impact an artifact of the much smaller evaluation datasets? \ud83e\udd14", "reply_to": null, "quoted": "1773002793520869711"}, {"id": "1773456761233805637", "text": "Would that explain the phase-transition behavior?", "reply_to": "1773071929231933544", "quoted": null}, {"id": "1773092418964910535", "text": "C4 validation loss increases steadily if slowly even after healing w. fraction of layers dropped tho \ud83e\udd14 Perhaps calibration (as hinted in 4.2) or other capabilities do require deep layers but they are not reflected in MMLU  &amp; BoolQ accuracies.", "reply_to": "1773002809211842739", "quoted": null}, {"id": "1773102729872753151", "text": "Yeah, I've found when probing that e.g. layers 25-60% of the way through have high \"info\" and the last layer has exceptionally high \"info\". The former being encoding-related (think edge-detection in vision models) and the latter obviously decoder related.", "reply_to": "1773002810402972130", "quoted": null}, {"id": "1773102816086626318", "text": "Still very shocking to me that the N-1 layer has so little information.", "reply_to": "1773102729872753151", "quoted": null}, {"id": "1773149521985348016", "text": "Have you guys tried evaluating the increase in loss when dropping layers? Cause in my experiments I notice usually the best prunable layers lies in middle of the second half ( 18-24th in mistral ) pruning the first layers hit the hardness", "reply_to": "1773002810402972130", "quoted": null}, {"id": "1773150728678199656", "text": "Some of the partial data I did on yi-6B model. In the end I could just cut it down by half the layers and still kinda usable", "reply_to": "1773149521985348016", "quoted": null}, {"id": "1773304890774728911", "text": "One thing that I would like to see in pruning papers like this is a comparison with a baseline scaling law with respect to model size. If you use the Chinchilla scaling law, training a model with 80% less parameters would only increase the loss from 1.78 to 1.85", "reply_to": null, "quoted": "1773002793520869711"}, {"id": "1773304893542936660", "text": "(note that the Chinchilla paper used C4 so this comparison makes sense. Also this is assuming infinite data, with finite data the reduction would be smaller). \nSo you would be substantially better off directly training a smaller model (although this might be much more expensive)", "reply_to": "1773304890774728911", "quoted": null}, {"id": "1773304893542936660", "text": "(note that the Chinchilla paper used C4 so this comparison makes sense. Also this is assuming infinite data, with finite data the reduction would be smaller). \nSo you would be substantially better off directly training a smaller model (although this might be much more expensive)", "reply_to": "1773304890774728911", "quoted": null}, {"id": "1773400764666335657", "text": "That's a very interesting point. Part of the perspective we were taking is, given an artifact that already exists, can we make it more efficient?", "reply_to": "1773304893542936660", "quoted": null}, {"id": "1773316251772703030", "text": "Interesting stuff. So why can't we just train a smaller network and get the same result? \n\nAlso the plot description seems to be exchanging the dark blue and light blue curves on sub-figure (d) - healing vs. no-healing", "reply_to": "1773002793520869711", "quoted": null}, {"id": "1773457603869478954", "text": "You're absolutely correct, thank you for pointing that out!", "reply_to": "1773316251772703030", "quoted": null}, {"id": "1776855156044317003", "text": "A variant of this question was posed in this paper. I think the TLDR is that some neural networks require weights to train and optimize, but those weights go unused during inference. \n\n", "reply_to": "1773316251772703030", "quoted": null}, {"id": "1773935560555413724", "text": "Really interesting paper. \n\nQ1: Is % where the performance drops in the chart indicative of model trained to capacity? phi/mistral may have been trained more relative to their size so less robust to layer drops\n\nQ2: Why not design lower layers to be wider than top layers then?", "reply_to": "1773002793520869711", "quoted": null}, {"id": "1774123828861350047", "text": "Thank you!\n\nA1: We also wondered how overtraining (or distillation) affects the ability to prune. We actually tested LLaMA(-1) models, and they had a phase transition at the same pruning fraction as Llama-2 models. So I'd say we're still not sure...", "reply_to": "1773935560555413724", "quoted": null}, {"id": "1774894673674313742", "text": "Really interesting read! There\u2019s something I\u2019m actually wondering\n\nIn the \u00ab\u00a0non-Qwen-case\u00a0\u00bb, you seem to have written that the deep layers appear to be quite similar\n\nWhat would specific fine-tuning only on those layers result to? Could it make them a specialist block?", "reply_to": "1773002793520869711", "quoted": null}, {"id": "1774895285421969725", "text": "It\u2019s a bit off-topic, as it\u2019s more restructuring than pruning, but if there are somewhat \u00ab\u00a0useless\u00a0\u00bb computation groups, perhaps they would be the most interesting layers either to perform continued pretraining/finetuning on, as a way to specialize the model", "reply_to": "1774894673674313742", "quoted": null}, {"id": "1774917689930093030", "text": "That's an interesting result and I think it's a common understanding that many LLM are seriously undertrained for their size. At the same time, the quality improvement when you pass from 7B to 70B parameters and when you pass from 70B to 1T parameters is also very small compared to the size increase. So, yes, you can cut some layers if you tolerate some drop in quality but then you can also quantize or just pick a smaller model. ", "reply_to": null, "quoted": null}, {"id": "1775145362916884745", "text": "Undertrained? Supposedly, sparse internally.", "reply_to": "1774917689930093030", "quoted": null}, {"id": "1775599577950790136", "text": "IMO, if the results show no performance degradation with extra deep layers on benchmarks, it doesn't imply these layers are useless\u2014just that they're not beneficial for the specific benchmarks tested. Or am I missing something   ?", "reply_to": "1775575925452603828", "quoted": null}, {"id": "1775600534239457280", "text": "any plans to open source the code? would love to try it out!", "reply_to": "1775575925452603828", "quoted": null}, {"id": "1775602051830956221", "text": "The code to drop layers is quite simple...", "reply_to": "1775600534239457280", "quoted": null}, {"id": "1775647318739464215", "text": "cool results but:\n\n1) scores not dropping on MMLU and BoolQ does not mean the model's abilities are not hurt, just that a subset of question answering ability is not hurt\n\n2) results *do* drop a little also for these datasets", "reply_to": null, "quoted": "1775575925452603828"}, {"id": "1775717794908094823", "text": "Thank you!\n\n1) I think we try to be careful and just say that its performance on QA isn\u2019t degraded. (We think the phase transition itself is interesting; the next-token prediction on the loss also has a phase transition if you don\u2019t heal, and then when you heal the loss increases linearly very slowly.) We\u2019d like to test on more benchmarks and will when we get a chance.\n\n2) For the similarity pruning method for, e.g. Llama-2 70B (Figure 1), they don\u2019t actually drop for the majority of pruning fractions before the phase transition. In general this seems to work better on larger models; we\u2019d like to test this much more extensively.", "reply_to": "1775647318739464215", "quoted": null}, {"id": "1775717794908094823", "text": "Thank you!\n\n1) I think we try to be careful and just say that its performance on QA isn\u2019t degraded. (We think the phase transition itself is interesting; the next-token prediction on the loss also has a phase transition if you don\u2019t heal, and then when you heal the loss increases linearly very slowly.) We\u2019d like to test on more benchmarks and will when we get a chance.\n\n2) For the similarity pruning method for, e.g. Llama-2 70B (Figure 1), they don\u2019t actually drop for the majority of pruning fractions before the phase transition. In general this seems to work better on larger models; we\u2019d like to test this much more extensively.", "reply_to": "1775647318739464215", "quoted": null}, {"id": "1775897500161114504", "text": "I think the real interesting insight will come when we figure out how to make training converge without over-parameterizing. This result, heavy quantization, double descent, training small models from large model instruction following all point to some related phenomenon.", "reply_to": "1775575925452603828", "quoted": null}, {"id": "1775907886217957444", "text": "Someone on here was saying that science studies phenomena and causality; which makes it predictive. The fact that there is little predictability in what would produce LLM's of what capabilities is telling of the basis of current GenAI tools.\n\nI agree with this perspective", "reply_to": "1775575925452603828", "quoted": null}, {"id": "1779569444790137019", "text": "I did some further experiments on layer pruning strategies e.g. ShortGPT or LaCO and i found that they were pretty good at \"making decision\" (MMLU, PIQA) but performed bad on generation (like MQAR or long-context generation for answering).", "reply_to": null, "quoted": "1772803686965694684"}, {"id": "1779569598825996637", "text": "One direct evidence is the drop on PPL", "reply_to": "1779569444790137019", "quoted": null}, {"id": "1787262585429004338", "text": "I believe that not only is this the wrong conclusion, but that we *need more* similar deep layers, executing subtle refinements in tasks stressing ICL and state tracking; and this is why Solar shines (Hermes-10.7B, FimbulbetrV2) and why I want Llama3-Solar.\n  disagrees.", "reply_to": null, "quoted": "1773002793520869711"}, {"id": "1787264516020957301", "text": "Are *you* team Depth or team Width? Do you think sparse upcycling makes more or less sense than depth upscaling?\n\nPractically: do you think there's more alpha in stretching Llama3-8B horizontally a la Mistral to Mixtral, or vertically to 48 layers, a la Mistral to Solar?", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787266562438611177", "text": "For the record, the reproduction of the study for L3 70b (the 42b that Charles Goddard did) didn't hold well outside of evals w/ 100 million tokens worth of qlora healing.\nWhether or not that's bc qlora is shallow, or bc more layers are truly needed, is a fascinating question", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787266788012556473", "text": "I'm really confused.  How does this square with the copying layers result from earlier? Seems like the are opposite conclusions?  What's the catch", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787265550889955359", "text": "imho this might be a sign that continuing training on more tokens is beneficial, it reads to me that there's still capacity to be used.", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787265550889955359", "text": "imho this might be a sign that continuing training on more tokens is beneficial, it reads to me that there's still capacity to be used.", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787266562438611177", "text": "For the record, the reproduction of the study for L3 70b (the 42b that Charles Goddard did) didn't hold well outside of evals w/ 100 million tokens worth of qlora healing.\nWhether or not that's bc qlora is shallow, or bc more layers are truly needed, is a fascinating question", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787268061927502162", "text": "Given the incentives, I take the absence of shortened models ('member Sheared llama?) in community usage as strong evidence that removing \"junk undertrained layers\" breaks something important.\n\nImo the key is understanding of learned self-correction/denoising. Attention sinks etc", "reply_to": "1787266562438611177", "quoted": null}, {"id": "1787266788012556473", "text": "I'm really confused.  How does this square with the copying layers result from earlier? Seems like the are opposite conclusions?  What's the catch", "reply_to": "1787262585429004338", "quoted": null}, {"id": "1787268319222874473", "text": "Which one", "reply_to": "1787266788012556473", "quoted": null}, {"id": "1788248183019323523", "text": "Interesting.. \n\nI still think the existing evals are not that great. Removing 40% of the layers surely have a bigger impact.\n\nBut interesting indeed.", "reply_to": null, "quoted": "1788224280754618393"}, {"id": "1788258657022714342", "text": "Maybe, but they eval with both MMLU and BoolQ and the math/mechanism for why this might work so well makes intuitive sense. In fact, the single most surprising thing to me after reading the paper is that this is the first thing of exactly this kind that I\u2019ve seen. They do cite another preprint that was published as they were in final edits. \n\nOne way to think about this is that our gradient descent techniques overall are still quite crude! There\u2019s lots of room for optimization beyond/outside the current standard approach to pre-training. \n\nPossibly this exact result is already well known inside one or more of the foundation labs.", "reply_to": "1788248183019323523", "quoted": null}, {"id": "1788364525713985538", "text": "Absolutely ... I'd love to see a BBH result", "reply_to": "1788248183019323523", "quoted": null}, {"id": "1788336341945569624", "text": "it might be the edge cases that might get lost like out of the box thinking or generalizing to new domains or maybe bad zero shot answers, but i can imagine a lot of values are close to zero and a lot of values might be cancelled so it could work as well", "reply_to": "1788248183019323523", "quoted": null}, {"id": "1788258657022714342", "text": "Maybe, but they eval with both MMLU and BoolQ and the math/mechanism for why this might work so well makes intuitive sense. In fact, the single most surprising thing to me after reading the paper is that this is the first thing of exactly this kind that I\u2019ve seen. They do cite another preprint that was published as they were in final edits. \n\nOne way to think about this is that our gradient descent techniques overall are still quite crude! There\u2019s lots of room for optimization beyond/outside the current standard approach to pre-training. \n\nPossibly this exact result is already well known inside one or more of the foundation labs.", "reply_to": "1788248183019323523", "quoted": null}, {"id": "1788277748105965761", "text": "It will reflect a bit the pre-training approach of the model. Dropouts and (eventually) layer pruning will make the result more robust against these layer drops. IMO \ud83d\ude09", "reply_to": "1788258657022714342", "quoted": null}, {"id": "1788265327446331541", "text": "This could be a sort of new Dropout layer during training that instead of removing neurons it reduces their precision\n\nOr even better, an activation function that has same shape as a quantization step", "reply_to": "1788234245611368582", "quoted": null}, {"id": "1788272450851230127", "text": "Hmm, I wonder how much can be lost without serious degradation of the model", "reply_to": "1788265327446331541", "quoted": null}, {"id": "1788331595608596681", "text": "1/2 Key realization:  a majority of  functions distinguishing AI model manifold limits don\u2019t require much complexity - Think high order terms/ extra neurons+weights in depth - but you don\u2019t know which ones you need a-priori. Easier to depend on gradient descent to train the model", "reply_to": null, "quoted": "1788224280754618393"}, {"id": "1788331598590734690", "text": "2/2 Just like human brains do, once trained, you can go back and eliminate all the unused higher order components in your model and regain lots of efficiency, power and storage requirement reduction, etc.\n\nThis is but one AI efficiency &amp; scale tricks we derive from neuroscience.", "reply_to": "1788331595608596681", "quoted": null}, {"id": "1788586061586727358", "text": "If you prune a model then it\u2019s not 70B anymore. Instead of pruning layers why don\u2019t you use non pruned LLM with lesser parameters?", "reply_to": "1788224280754618393", "quoted": null}, {"id": "1789285140864102411", "text": "i wonder why duplicating middle layers is most common in frankenmerges when the least information-dense ones are towards the end, according to this paper? you'd think doing the earlier ones would yield better/more noticable/whatever results?\n", "reply_to": null, "quoted": null}, {"id": "1789285432510783761", "text": "of course this can be answered by \"frankenmerges are bullshit that doesn't work\" but that's no fun", "reply_to": "1789285140864102411", "quoted": null}, {"id": "1789393649530482815", "text": "True. And important to keep in mind.\n\nIn this case, though, if specific layers provide very little value to the output, evaluated quantitatively in the context of two different standard measures, why would we assume that the layers add value in other eval contexts?\n\nOr are we suggesting that the base model was already overfitting, so these measures are just bad for *any* evaluation of llama-2 (and the other models used in the paper)?\n\nA bunch of people have posted responses to this paper that are similar. And this take may be correct!\n\nBut to me, the Occam\u2019s razor intuition here is that our training techniques are still pretty basic and might tend to, for example, concentrate the useful activations in surprisingly small subsets of model weights. \n\nMaybe most of our \u201cdense\u201d SOTA LLMs today are, in fact, MOEs that we forgot to add routers to. :-)", "reply_to": "1789389555818172520", "quoted": null}, {"id": "1818538060633514011", "text": "i agree with the principle but this paper is misleading\nin practice, any well fit model completely loses its state tracking abilities when you purge the deepest layers specifically\nwe tried training to heal the pruned l3 70b-42b, it didn't work\ndepth-wise pruning is *not* the way", "reply_to": "1818536457935479097", "quoted": null}, {"id": "1818538981534949866", "text": "What\u2019s meant by state tracking abilities? The fact that token cosine similarity between layers changes so little is a pretty convincing case no?", "reply_to": "1818538060633514011", "quoted": null}]