[{"id": "1802638141997621584", "text": "This business about copyright is ruining AI. I want my AI to know the works (books, poetry, reports, laws, thesis, scientific papers, etc.) verbatim, not approximately. Removing memorization would introduce errors and misinterpretations. Do we really need to cripple AI for money?", "reply_to": "1802629872503976057", "quoted": null}, {"id": "1802638535465324742", "text": "interesting. then how it even learns anything?", "reply_to": "1802629872503976057", "quoted": null}, {"id": "1802643981869662246", "text": "It still does the backward pass on most of the tokens, just not all of them. So still plenty of opportunity to learn", "reply_to": "1802638535465324742", "quoted": null}, {"id": "1802640297685377455", "text": "Great title! Question: so its better at not memorizing, but do they show that the non-memorization leads to better actual understanding of the material? (I'll read the paper soon!)", "reply_to": "1802629872503976057", "quoted": null}, {"id": "1802729493624488185", "text": "How does it affect the \"hallucination\" rate?  I ask because I suspect that a lot of current use cases (specifically the ones troubled by \"hallucinations\") implicitly rely on memorization of the source material at some level.", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802730563478106521", "text": "These users are expecting the LLM to have learned the training corpus and not just modeled language: they don't just want coherent prose, they want so called factually correct information (where \"factually correct\"\u2261\"found in the training data and generally believed to be true\").", "reply_to": "1802729493624488185", "quoted": null}, {"id": "1802733857474551969", "text": "Interesting. Any thoughts on how it responds to widely replicated texts? eg cliched phrases, song lyrics, common licenses? How much repetition is needed to undermine the goldfishing?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802742151710724466", "text": "We use a hash-based scheme that prevents memorization even if a document is duplicated many time in the train set.  TL;DR We hash a window of tokens and use the hash value to (deterministically) choose whether or not to drop a token inside the window.  For this reason, a paragraph of text will have the same tokens dropped every time it appears in the train set, even it appears surrounded by different context.", "reply_to": "1802733857474551969", "quoted": null}, {"id": "1802736598875832466", "text": "You could combine this idea with token-wise RhoLoss (see also Rho-1) and only select the tokens that are worth learning. This would likely also prevent overfitting as you argue while preserving training speed", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802740906291786078", "text": "This is a great idea.  The Rho paper came out after we had run a lot of our experiments.  We discussed this a lot but we decided not to go back and re-run (there's only so much our GPUs can handle).  There could be a free lunch - No memorization without a drop in learning speed.  Would be super cool if that's true, but I don't know.", "reply_to": "1802736598875832466", "quoted": null}, {"id": "1802762420626030988", "text": "Very interesting idea. Yet, it feels like something that increases hallucinations in LLMs, which is a main challenges in utilizing LLMs in real world applications (and not just creative writing). I wonder if you checked this as I think that some information must be memorized", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802762741184082302", "text": "LLM memorization is one of its useful features, as crystallized knowledge is a proxy for intelligence.\n\nWon't removing this feature make them dumber?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802766559904940478", "text": "What happens if the text appears shifted in the dataset?\nE.g.\n\"Mr. and Mrs. Dursley [...]\"\n\"Chapter 8: Mr. and Mrs. Dursley [...]\"", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802790989523779714", "text": "The mask is chosen based on a local hash of the h preceding tokens, so that h controls how granular the mask applies to different contexts", "reply_to": "1802766559904940478", "quoted": null}, {"id": "1802780869662327030", "text": "A solution could be to decide whether to use the loss of the current token based on a hash value of the previous n tokens with  n being a new hyperparameter, together with the decsion threshold applied to the hash value.", "reply_to": "1802766559904940478", "quoted": null}, {"id": "1802790361154167017", "text": "That's precisely what we do :]\n", "reply_to": "1802766559904940478", "quoted": "1802788931911717130"}, {"id": "1802769219253088474", "text": "This is great for natural language, but for code, I think this might not give the best quality or give less working code on average. Code even when copyrighted, have only so many ways of doing the same thing, unless you obfuscate the code, or rename variables, etc. There not really a concept of \"synonyms\" in code, so this might not really work there, IMO", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1803062072567054510", "text": "I think it will find its way to correct code anyway, just not verbatim. \n\nYou want to encode concepts, not memorize sequences. If a concept has other constraints than the source text, those will still force it in the right direction. \n\nThat's my layman understanding at least.", "reply_to": "1802769219253088474", "quoted": null}, {"id": "1802800497616965831", "text": "Fascinating!! congrats to the team\n\nhave you tested for in-context learning tasks? what about tasks like 'repeat after me'?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802806826553274746", "text": "have you guys done any experiments for finetuning? Will it help pretrained models mitigate memorization issue?", "reply_to": "1802629872503976057", "quoted": null}, {"id": "1802808821703963058", "text": "do you know if it works for finetuning to help pretrained models mitigate memorization issue?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802889666883997977", "text": "We do experiments with both pre-training and fine-tuning.  It seems to work similarly in both situations; memorization is suppressed.  In the SFT case we did not notice the \"slowdown\" that observed for pretraining, athough I think that a more sensitive test with larger SFT and evaluations sets would.", "reply_to": "1802808821703963058", "quoted": null}, {"id": "1802836308395725093", "text": "Does your method makes the model smarter?", "reply_to": "1802726890236477710", "quoted": null}, {"id": "1802859839275638859", "text": "I don't think so. In some strict sense the model gets less information than when training using standard loss. (speculation) However, it is possible that the reduction in ability to regurgitate sequences could change generalization characteristics under certain test settings\ud83e\udd37\ud83c\udffc", "reply_to": "1802836308395725093", "quoted": null}, {"id": "1802843274660728895", "text": "Wow!\n\nWhat do people think of this?\n\nAm I crazy for thinking it's big/huge?\n\nMemorization vs generalization has always been the biggest problem with just doing tons of epochs.", "reply_to": null, "quoted": "1802726878924464273"}, {"id": "1802871298718347519", "text": "How does this affect its ability to learn famous quotes and other non-copyrighted text that is expected to be reproduced exactly? e.g. \"One small step for man...\"", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1802889076795957309", "text": "Goldfish would prevent those from being memorized.  For this reason one would probably choose not to apply goldfish loss to public domain documents, wikipedia, etc...", "reply_to": "1802871298718347519", "quoted": null}, {"id": "1802882709825716462", "text": "Sorry if a naive question, but can something like this be used to mitigate \u201cforgetting\u201d when fine tuning a base model or is that subject to a completely different mechanism?", "reply_to": "1802726888453919199", "quoted": null}, {"id": "1802920724648038445", "text": "Simple and cool idea to reduce (to fair use?) issues of plagiarism and copyright infringement of the training text \u2014 reduce model memorization already during training instead of unlearning at a later stage", "reply_to": null, "quoted": "1802726878924464273"}, {"id": "1802920727936438626", "text": "It would be interesting to see how well it can do on image generation, video and audio", "reply_to": "1802920724648038445", "quoted": null}, {"id": "1802923639471255643", "text": "I wonder if this also would help models generalize more as memorisation would no longer be as good a working strategy?", "reply_to": null, "quoted": "1802726878924464273"}, {"id": "1802980848553398283", "text": "Do you expect this to work: if you first train with standard loss fully then continuing training with goldfish loss for another epoch.", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1803127866823155909", "text": "l(token1)-alpha*l(token2)+l(token3) +l(token4), might be worth trying negative losses to forget what\u2019s learned from the first epoch.", "reply_to": "1802980848553398283", "quoted": null}, {"id": "1802983953693069749", "text": "is the excluded token fixed?  For eaxmple, 4 sequent tokens are \"this is Jack\" and in the first cycle, \"this\" is excluded. How about the following training round, is \"this\" all excluded or just random one of them is excluded.", "reply_to": "1802726880660869214", "quoted": null}, {"id": "1802987764448698756", "text": "Cool work!! I dont know much about nlp\u2014how do you balance memorization needs in practice?", "reply_to": "1802728967662862620", "quoted": null}, {"id": "1803052660163002608", "text": "Thanks Laura! Memorization is important for retaining facts and famous quotes but where you don\u2019t want to memorization is where the text is copyrighted, etc.\n\nThis is awesome work lead by ", "reply_to": "1802987764448698756", "quoted": null}, {"id": "1803037094018678924", "text": "More and more works focus on preventing copyright infringement risks, especially preventing the model from memorization. Can these methods still work when the model can retrieve the gold document and use it as a reference during generation (i.e. no need to memorize)?", "reply_to": null, "quoted": "1802726878924464273"}, {"id": "1803076575170334938", "text": "In that case, it is more similar to search engines? Then, the problem is not to prevent models from outputting verbatim text, but instead ensuring models to note the source of references to give them acknowledgment?", "reply_to": "1803037094018678924", "quoted": null}, {"id": "1803076575170334938", "text": "In that case, it is more similar to search engines? Then, the problem is not to prevent models from outputting verbatim text, but instead ensuring models to note the source of references to give them acknowledgment?", "reply_to": "1803037094018678924", "quoted": null}, {"id": "1803078748914479163", "text": "This is one possible mitigation. Another way is to intervene during the decoding time when we detect that the model is generating the blocklisted content. However, I don't think any methods can perfectly achieve this goal.", "reply_to": "1803076575170334938", "quoted": null}, {"id": "1803079701466095794", "text": "From my perspective, identifying copyright infringement is not limited to exact matching. Near duplicates, high semantic similarity may also need to be considered in some cases. Reducing all of them is not an easy thing, not to mention balancing utility and efficiency.", "reply_to": "1803076575170334938", "quoted": null}, {"id": "1803144993315627471", "text": "\ud83e\udd14What if multiple copies of the same text with different first token are in the training set? Let's say you skip every other token.  You can learn Token 1,3,5,7, 9 in one copy and Token 2,4,6,8,10 in another copy.  In the end, it could still memorize the whole thing?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1803593567077802057", "text": "Great question! To handle the case where the document appears in non-identical forms we use a local hashing scheme (Section 3.1 might be of interest to you).  let me know if you have more questions.", "reply_to": "1803144993315627471", "quoted": null}, {"id": "1803170932606779620", "text": "This has a nasty side effect of making a model worse for the same compute + data.", "reply_to": "1802726890236477710", "quoted": null}, {"id": "1803475630920552915", "text": "We analyze this predictable tradeoff in Figure 5. No lunch is perfectly free, however, we do discuss that in any real deployment scenario, one might only employ the Goldfish loss on specific documents or subsets of the data where regeneration is particularly undesirable.", "reply_to": "1803170932606779620", "quoted": null}, {"id": "1803358047940575516", "text": "Protecting privacy in LLMs has little to do with stopping full replication of strings. LLMs gather and connect related information and make it queryable, and produce opinions. Both are personal data when connected to identifiable individuals regardless of memorisation of phrasing", "reply_to": null, "quoted": "1802726878924464273"}, {"id": "1803359520350990344", "text": "(i know what they actually do is predict the next token, but \u2018opinions\u2019 and queryable information are legal concepts rather than comments on the most faithful and lucid characterisation of LLM functions)", "reply_to": "1803358047940575516", "quoted": null}, {"id": "1803404252934533449", "text": "Personal data finally reveals itself as a kind of observable distinction", "reply_to": "1803358047940575516", "quoted": null}, {"id": "1803392356835545184", "text": "What does this do for factual accuracy for known content / hallucination rates though?", "reply_to": "1802726878924464273", "quoted": null}, {"id": "1872812104371278050", "text": "Fascinating findings! How does this impact the model's generalization ability?", "reply_to": "1802788928879288426", "quoted": null}]