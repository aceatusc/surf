[{"id": "1661416550132482048", "text": "Nice! I guess the natural question is when the diagonal hessian approximation by gradients break down?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661418401720565766", "text": "yes, i think that would be an interesting question to investigate!", "reply_to": "1661416550132482048", "quoted": null}, {"id": "1661431317341298690", "text": "How does it compare to LION?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661488280481959936", "text": "We have some comparisons with Lion on 125M and 335M. E.g., figure 5b and Figure 4a,b, and Figure 7b. As far as we understand, the Lion paper doesn't claim significant improvements for LLMs and we didn't find big improvement either.", "reply_to": "1661431317341298690", "quoted": null}, {"id": "1661441923914104846", "text": "What's LION, and what is better than Adam?", "reply_to": "1661431317341298690", "quoted": null}, {"id": "1661432639570542602", "text": "So how does this affect scaling optimums. it seems we need less data now, or smaller models, to reach the same quality, but perhaps the optimum remains same data to compute ratio?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661433346444894209", "text": "The paper specifically focuses on GPT-2 models and language modeling tasks, so it's unclear how well Sophia would perform on other models or tasks, further experimentation and evaluation on diverse benchmarks would be necessary", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661457714533511168", "text": "yes, we only focus on LLMs. We will scale up the LLMs experiments by using up all our compute budget on this, perhaps will have to rely on the community's help to see if it also works for other tasks/models.", "reply_to": "1661433346444894209", "quoted": null}, {"id": "1661434417997381648", "text": "Also the sensitivity to hyperparameters part , improper setting of hyperparameters such as learning rate, clipping thresholds, or the frequency of estimating the diagonal hessian could lead to suboptimal results or even instability during training", "reply_to": "1661433346444894209", "quoted": null}, {"id": "1661439434439680001", "text": "How much memory does it use compared to Adam?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661462273075085318", "text": "It uses the same memory as Adam. Thanks for your interest!", "reply_to": "1661439434439680001", "quoted": null}, {"id": "1661439832613335040", "text": "The same", "reply_to": "1661439434439680001", "quoted": null}, {"id": "1661442822493405196", "text": "To answer other questions in the replies, I found the memory usage. It's the same as AdamW and the compute is about 5% more (for the optimizer only).\n\nIf you reduce training by 2x \u2014 or even just 1.1x \u2014 should be easy to justify switching!", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661761831294173186", "text": "Is the comparison here wrt fused Adam or wrt to naive adapt with a python loop? That's a big difference", "reply_to": "1661442822493405196", "quoted": null}, {"id": "1661444929586089984", "text": "so 10% (k=10) of steps are used for hessian estimation, but no parameter updates?", "reply_to": "1661413000215920655", "quoted": null}, {"id": "1661446121384980482", "text": "Thanks for the great work on the exciting 2nd-order optimizer!  \n\nI found Lion has the same lambda as AdamW while with a 5x smaller lr. Sec 5 of our paper shows that it should be increased to maintain a similar strength.  \n\nWould be interested to see the results of tuned Lion.", "reply_to": null, "quoted": "1661412995430219786"}, {"id": "1661446123909955585", "text": "Section 5 in the Lion paper ( )", "reply_to": "1661446121384980482", "quoted": null}, {"id": "1661453606049173504", "text": "This is quite cool! From a skim, it looks like the most significant insights here are: a) you only need to re-estimate the Hessian once every k steps and b) we really need to handle the saddle points and rapid curvature changes better. Does that sound right? \ud83e\uddf5", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661457075841662976", "text": "Yes, and in some sense handling b) well (with the simple element-wise clipping) allows you to do a) without any blowing-up.", "reply_to": "1661453606049173504", "quoted": null}, {"id": "1661453667822874624", "text": "For a) cool! That\u2019s very interesting, I wonder how it carries over to the classical nonlinear optimization setting (somewhere between chord and BFGS maybe?)", "reply_to": "1661453606049173504", "quoted": null}, {"id": "1661455300132569088", "text": "Are you using fancy schedulers with Sophia?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661456481269719040", "text": "Do you mean learning rate schedules? We are using the  standard cosine LR which has an initial warmup period, and decays 1/10 of the peak LR eventually. The LR schedule is also plotted in Figure 5a.", "reply_to": "1661455300132569088", "quoted": null}, {"id": "1661470521228402688", "text": "How does this compare to other second-order optimizers like K-FAC, Shampoo, KAISA, AdaHessian, etc?", "reply_to": "1661413008151838720", "quoted": null}, {"id": "1662919996941688832", "text": "K-FAC &amp; AdaHessian are discussed in the paper briefly. K-FAC estimates block-diagonal Hessian (instead of diagonal) so more expensive. AdaHessian estimates diagonal Hessian at every step and doesn't do clipping.\n\nSophia's Gauss-Newton-Bartlett estimator is also new, I think.", "reply_to": "1661470521228402688", "quoted": null}, {"id": "1661478134481584129", "text": "How does it do on computer vision applications, either Vision Transformers or conventional but large CNN based ResNets?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661488981106884608", "text": "Unfortunately we've used all of our tiny compute resource on LLMs. We will release the code very soon and hopefully someone will try it out.", "reply_to": "1661478134481584129", "quoted": null}, {"id": "1666340891596926977", "text": "I'm currently trying it on Vision Transformer for Single Object Tracking with 2 trainings with each of the optimizers (AdamW or Sophia), training with AdamW seems faster (for the moment). But there's a lot more training to go", "reply_to": "1661478134481584129", "quoted": null}, {"id": "1661478339616579584", "text": "This looks interesting, I wonder if these results hold for non LLM based applications like Vision Transformers or large, conventional CNN based ResNets?", "reply_to": null, "quoted": "1661412995430219786"}, {"id": "1661493731273629696", "text": " Are there any results for fully trained models i.e., at full convergence ?. Many times in the past optimizers which tend to work well in the initial phases of the learning compared to other optimizers, tend to underperform at convergence .. thanks", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661515398100160512", "text": "I've always been told that training Transformers requires expertise to the specific architecture one is optimizing. How robust does this new optimizer work across Transformer architectures?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661520609510322176", "text": "I only tested on the standard decoder transformers on GPT-2. So far we see some improved stability because we need fewer tricks to make it work, but more thorough tests of course are needed.", "reply_to": "1661515398100160512", "quoted": null}, {"id": "1661519600599859200", "text": "The challenge of course is that the real curvature is intractable so the comparison is between a structured approximation to the Hessian which is often a very poor fit.", "reply_to": "1661506276323622912", "quoted": null}, {"id": "1661533987691696130", "text": "You can do unstructured Hessian diagonal estimation using Hutchison's. Variance goes down as 1/samples, same as for stochastic gradient estimation", "reply_to": "1661519600599859200", "quoted": null}, {"id": "1661525439746064384", "text": "Interesting work. I'm going to read the entire paper tonight, but I've been thinking a lot about the extensive body of prior work in optimization.\nA meta-optimizer that parameterizes over the body of prior work seems like potentially fruitful research. (One hopes that fine-tuning on small LMs generalizes to much larger LLMs, but I'm not aware of much research in this area.)", "reply_to": "1661421373078781954", "quoted": null}, {"id": "1661553323688730626", "text": " is this the optimizer u were talking about?\n\nwould be very interested to see how it performs on vision tasks + what math insight it provides", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661556253464592385", "text": "Yep yep - and I\u2019m not sure if the Hessian estimate gives good results in general + not sure what other priors it has (if any)", "reply_to": "1661553323688730626", "quoted": null}, {"id": "1661583049640013824", "text": "why no Sophia-G learning curve for the large model?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661588048105345024", "text": "The truth is that the experiment is still running \ud83d\ude02 But we currently believe Sophia-G is generally better than Sophia-H. Will update soon! Thanks!", "reply_to": "1661583049640013824", "quoted": null}, {"id": "1661602630135873536", "text": "The method is fascinating, but the claim that Sophia is 2x faster than Adam is misleading imo. Fig 5(c) shows that AdamW with 100K iters also achieves almost the same valid loss as AdamW with 200K iters, so the speedup is mainly due to the lr schedule (not the optimizer change).", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661603823088173058", "text": "The same applies to Fig 9 in the appendix.", "reply_to": "1661602630135873536", "quoted": null}, {"id": "1663464432372948995", "text": "Based on   remark, indeed, this representation can mislead. At least, for the moment, I do not understand, why Sophia should be better than AdamW?", "reply_to": "1661602630135873536", "quoted": null}, {"id": "1661613294845833217", "text": "the weight decay 0.1 for all experiment seems pretty high, and also Sophia-G lr seems all over the place, and usually order of magnitude smaller than Adam, how do we tune them?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661622571052470272", "text": "That's a great question about the LR. We are aiming to release the code tomorrow, where we use an equivalent, but different definition of LR, which was what we used in code and is in standard range. The LR def used in paper is cleanest for writing but indeed not the best for code", "reply_to": "1661613294845833217", "quoted": null}, {"id": "1661616364354785280", "text": "why is the comparison not done with the dotted 100K-AdamW line for the speedup?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661790422027636736", "text": "That's because the 100K-AdamW dotted line doesn't achieve the same loss (and one can't continue running it because the number of steps is pre-decided to be 100K, which also decides the LR schedule.)", "reply_to": "1661616364354785280", "quoted": null}, {"id": "1661637829364162561", "text": "Is this natural gradient with diagonal approximation to Fisher? FWIW some implementations of KFAC do EMA estimates of the preconditioner with infrequent updates.", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661789887362899968", "text": "I guess some differences from k-fac are: (a) the element-wise clipping, (b) the GNB estimator is actually \"new\" (not used in k-fac)---please see Sec 2.3 of the paper for some contexts. E.g., eqn (11) was considered not implementable before.", "reply_to": "1661637829364162561", "quoted": null}, {"id": "1661787848335568896", "text": "What is the reason for only testing Sophia-H on GPT-2 Large, but not Sophia-G?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1661791237756665856", "text": "The experiment for Sophia-G on 770M is still running\ud83d\ude02---so it's not like we intentionally exclude it.  Our experience on smaller models is that Sophia-G is generally better than Sophia-H. Thanks!", "reply_to": "1661787848335568896", "quoted": null}, {"id": "1661917848933892097", "text": "Does it only work with categorical logits? I am working on a regression task using LLM, can I use it and how to?", "reply_to": "1661413000215920655", "quoted": null}, {"id": "1661998498022293508", "text": "I think it should work because that\u2019s also an exponential distribution. Feel free to email us \u2014it\u2019s perhaps easier to discuss over emails.", "reply_to": "1661917848933892097", "quoted": null}, {"id": "1662345354808029184", "text": "One thing I'm curious about: how can we extend this Hessian estimator to diffusion models, where the loss is typically a simple MSE term? I'm assuming there's a pretty trivial way to do it since MSE grad looks similar to softmax's; curious for your thoughts.", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1662409421396271104", "text": "Congrats for your great work! In text before equation (9), it says the S is not dependent on y, then why should we add the expectation and do the sampling again? If y makes no difference, I suspect we can just use the ground truth label. What is the difference here?", "reply_to": "1661791726208716800", "quoted": null}, {"id": "1662868174436548615", "text": "Interesting results! Regarding the analysis, Eq. (15) looks more like a clipped version of Newton's method, has this not been analyzed before? I'm mostly aware of clipped GD  \nAssumption 4.2 looks a bit unconventional too, any justification for it?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1663973986701590535", "text": "and also quick question (sorry for multiple questions, I think your work is very exciting). Did you use somehow the specific LLM architectures in the algorithm/its implementation? Or it is equally good for other ML models?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1668350934727151616", "text": "The comparison seems unfair for Adam. I wonder what will happen if you use Adam as the hessian estimator in Sophia (basically AdamW + lazy k-step update of 2nd moment + clipping of 1st moment / 2nd moment). I suggest use the same bag of tricks of sophia on AdamW", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1714937704725479437", "text": "Since this potentially could save millions (or billions) in training compute $ for large commercial companies like OpenAI, it seems fair to ask them for some form of fee or reinvestment into academia in exchange for non-academic access to Sophia. What do you think?", "reply_to": "1714708089205912004", "quoted": null}, {"id": "1714938554361537014", "text": "This goes back to the larger issue of public AI work and investment leading to exclusively private profits. Would some form of foundation between all AI-researching universities be a solution, where researchers could send their results too, and companies", "reply_to": "1714937704725479437", "quoted": null}, {"id": "1716909635310068160", "text": "Do you know what's the cause for step-like plot behavior for this particular experiment?", "reply_to": "1714709649294061719", "quoted": null}, {"id": "1768140639454089704", "text": "Can you give some intuition about the non-standard HP choices of b1=.96 and b2=.99, eps=1e-12? (AdamW LLM default is b1=.9 b2=.95, eps=1e-8).", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1768172818485903797", "text": "Many people were not able to reproduce I thought?", "reply_to": "1767966654120735158", "quoted": null}, {"id": "1768173816940007568", "text": "Well, one of the nice things about Levanter is it's bitwise reproducible, so you can definitely get the same results I get... I can't promise it's going to help in every situation.", "reply_to": "1768172818485903797", "quoted": null}, {"id": "1768237426449764709", "text": "How does speed per step compare to Adam? Is it also faster when looking at training time, and not just training steps?", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1768303606074139135", "text": "How is it with small data? (i.e not just LLM/pretraining scale data + models, nvm tabulardata?)", "reply_to": "1661412995430219786", "quoted": null}, {"id": "1769135181489639424", "text": "Any speculation as to why sophia tends to like higher beta1? Maybe because the hessian is tightly controlling/stabilizing so higher momentum increases exploration of the loss landscape?", "reply_to": "1767966654120735158", "quoted": null}]