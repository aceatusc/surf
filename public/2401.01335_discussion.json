[{"id": "1742366501305307365", "text": "More evidence that AIs are not going to be limited by the amount of human-created content available for them to train on.\n\nThis is another paper suggesting that AIs training on AI-created data can achieve higher quality results than just using human-created data alone.", "reply_to": null, "quoted": "1742363796557926663"}, {"id": "1742376389029113976", "text": "I was reading something similar (I've attached the link) but it seems a little doubtful considering AI-generated data is a result of training on human-generated data. \n\nIt can give more non-sensical or odd responses as shown in the paper.\n\n", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742906664703971391", "text": "No, what this shows is that you can distil GPT-4 knowledge into smaller models\n\nThis is not \"self-play\" or \"self-generation\" and the knowledge still comes from human-generated and human-curated GPT-4 training data", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742705047346298896", "text": "What\u2019s next? Evidence that humans can learn from human created content too?\n\nRidiculous.\n\nWhat is this some kind of perpetual motion machine? Creating something from nothing?", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742373382434226381", "text": "Idk if you look at the examples in the paper they\u2019re pretty weak (even/odd months?). There\u2019s a lot of lack of clarity in the initial English prompts, I think some of it is having ChatGPT just rephrase to standard edited English", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742374062955917707", "text": "If we had an LLM in the early 1600s, would it tell us that the earth or the sun was in the center of our solar system?\n\nDoes AI created data get us closer to its answer being \u201cthe sun\u201d or \u201cthe Earth\u201d?\n\nFeels like the latter sadly", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742549516476150074", "text": "I wonder how this aligns with research indicating that training an LLM on LLM generated content degrades the quality of it.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742502177115942917", "text": "So AI needs confirmation bias?", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742468481847640359", "text": "Yesterday, ChatGPT 4 was stumped on an Excel formula. I asked about it in a separate conversation (with Copilot) and got a polite indication of a way forward, which CHATGPT 4 promptly managed to use to overcome its limitation. And it's the same model behind both, it's crazy.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742465345913295126", "text": "Cuuurick", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742370826299539547", "text": "Incredible.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742378222870999166", "text": "Makes sense. My brain as a human was trained on language from other humans, which was trained on language from other humans.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742602573817713083", "text": "Lex Alpha Zero", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742420562759049467", "text": "This seems great but I guess I am at a loss for why human annotated data should be in short supply. If we offer people individualized LLMs that are trained based on basic star-rated feedback people will have the incentive to train the LLM.\n\nThen you cross breed those LLM by having them serve as teachers to each other. The resultant daughter LLM is offered to the user as an update. If it's no better the user rejects the update, that cross breed is eliminated and its teacher's \"fitness\" is dinged. Inverse if update is accepted.\n\nMore fit teachers are bred with higher probability increasing fitness across the whole ecosystem.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742487474545311982", "text": "If the AI-generated data is self-checking, self-correcting based on first principles, we're in for a new enlightenment.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742417895576264838", "text": "I\u2019ll be there judge.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742708388797325484", "text": "This is how fake and untruths will eventually become truths and law !!! Idiocracy is being  created slowly", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742373382434226381", "text": "Idk if you look at the examples in the paper they\u2019re pretty weak (even/odd months?). There\u2019s a lot of lack of clarity in the initial English prompts, I think some of it is having ChatGPT just rephrase to standard edited English", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742376389029113976", "text": "I was reading something similar (I've attached the link) but it seems a little doubtful considering AI-generated data is a result of training on human-generated data. \n\nIt can give more non-sensical or odd responses as shown in the paper.\n\n", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742390211319906515", "text": "This might be the biggest result in finetuning methods that the community can realistically use yet (assuming it holds). Reminds me of BYOL-Explore and other pull-yourself-by-your-bootstraps RL breakthroughs. So much to think about. ", "reply_to": null, "quoted": "1742385193887953014"}, {"id": "1742409056810058069", "text": "Looks very interesting! I wonder what is the intuition, on a high level, that given a fixed SFT dataset, doing self-play is better than simply doing SFT, which seems to be counter intuitive.", "reply_to": "1742372246868623758", "quoted": null}, {"id": "1742622136454480292", "text": "Thanks! We observed is that even after fine-tuning the LLM on a given SFT dataset, there is still a noticeable quality gap between its response to a training prompt and the ground truth completion. At high level, SPIN is aimed to let the LLM itself discern the gap and self-improve to generate responses as close as possible to the high-quality ground truth completions in the SFT dataset.", "reply_to": "1742409056810058069", "quoted": null}, {"id": "1742421294770016683", "text": "Same intuition for DPO (tell me when you figure this out lol)", "reply_to": "1742409056810058069", "quoted": null}, {"id": "1742422008971862060", "text": "More data is one factor since you generate as many samples as you want.", "reply_to": "1742409056810058069", "quoted": null}, {"id": "1742420562759049467", "text": "This seems great but I guess I am at a loss for why human annotated data should be in short supply. If we offer people individualized LLMs that are trained based on basic star-rated feedback people will have the incentive to train the LLM.\n\nThen you cross breed those LLM by having them serve as teachers to each other. The resultant daughter LLM is offered to the user as an update. If it's no better the user rejects the update, that cross breed is eliminated and its teacher's \"fitness\" is dinged. Inverse if update is accepted.\n\nMore fit teachers are bred with higher probability increasing fitness across the whole ecosystem.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742545071910596833", "text": "That kind of preference data isn't very helpful since it's low resolution. Like if an answer is 3/5 stars it doesn't come with an explanation for which parts are bad or factually incorrect.", "reply_to": "1742420562759049467", "quoted": null}, {"id": "1742549516476150074", "text": "I wonder how this aligns with research indicating that training an LLM on LLM generated content degrades the quality of it.", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742574317785714820", "text": "What's the minimum requirement for effective fine tuning with the core LLM? E.g., starting with Llama2 7b?\n\nThere's huge potential/need for efficiencies to overcome global lack of compute, including making edge applications more powerful. This FT concept could aid this area.", "reply_to": "1742570414226714967", "quoted": null}, {"id": "1742606493445390672", "text": "Cool new work! I think an alternate title to the paper can be \"Generative Adversarial Imitation Learning for LLMs\". A bit related to our recent work on self-training, but removes the need for having access to an oracle reward function (by using human demonstrations + discriminator instead). Curious to see how it might at work at larger scales / on harder reasoning tasks such as Hendrycks MATH.", "reply_to": null, "quoted": "1742372246868623758"}, {"id": "1742621085898768796", "text": "Thank you. While SPIN may share a similar spirit with GAIL, it is fundamentally different. In contrast to GAIL, which involves training separate discriminator and policy networks in each iteration, SPIN relies on self-play. In SPIN, both the main player and the opponent player are the same LLM from two consecutive iterations, streamlining the training process to involve only one neural network per iteration.", "reply_to": "1742606493445390672", "quoted": null}, {"id": "1742618356610629685", "text": "Cool work! Though w/o external feedback, do the self-play models plateau at the quality of the SFT data?", "reply_to": "1742369496420213182", "quoted": null}, {"id": "1742619101334573086", "text": "Thank you! I think the self-play fine-tuning does tend to reach a plateau in quality aligned with the characteristics of the SFT data in the absence of external feedback.", "reply_to": "1742618356610629685", "quoted": null}, {"id": "1742632622269005881", "text": "", "reply_to": "1742618356610629685", "quoted": null}, {"id": "1742658960283533634", "text": "Could be an important paper, though I don't grok this part: \"the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data\"", "reply_to": "1742372102940839937", "quoted": null}, {"id": "1742928631167648036", "text": "Answering my own question: it compares against the original \"human-annotated SFT dataset\". The iterative / adversarial approach is a clear win. I'm skeptical of calling it 'synthetic data' but maybe I don't understand how researchers use that phrase.", "reply_to": "1742658960283533634", "quoted": null}, {"id": "1742810751323226355", "text": "what happens when you continue self-play for the same number of tokens to equivalent to an epoch? Does it continue to improve performance?", "reply_to": "1742661596248449152", "quoted": null}, {"id": "1742813085965197756", "text": "Interesting work! You mentioned that Self-play is similar to the idea of GANs, additionally, when performing the iterations did you face similar problems to training GANs - Mode collapse, instability while training and high sensitivity to hyperparams?", "reply_to": "1742661587436216615", "quoted": null}, {"id": "1742947845605605756", "text": "The fundamental difference of SPIN is that the main player and the opponent player are the same LLM from two consecutive iterations, rather than two separate NNs. Therefore, only one parameter \\btheta will get updated, which makes the training stable.", "reply_to": "1742813085965197756", "quoted": null}, {"id": "1742874389417517228", "text": "Seems like table 5 and 6 at the end has the wrong prompts?", "reply_to": "1742661587436216615", "quoted": null}, {"id": "1742943390894117222", "text": "Thank you for pointing this out! The prompt of tables 5 and 6 should be swapped. We will rectify it in the revision.", "reply_to": "1742874389417517228", "quoted": null}, {"id": "1742906664703971391", "text": "No, what this shows is that you can distil GPT-4 knowledge into smaller models\n\nThis is not \"self-play\" or \"self-generation\" and the knowledge still comes from human-generated and human-curated GPT-4 training data", "reply_to": "1742366501305307365", "quoted": null}, {"id": "1742936864657707068", "text": "How do you avoid overfitting this ranking model? Why doesn't the model fixate on superficial features and shortcut-learn?", "reply_to": "1742661591307559284", "quoted": null}, {"id": "1743011648699285829", "text": "Hi, this is amazing work. \nOne question, did you use the alpha or beta for zephyr-7b-sft? The sft model in the zephyr-7b paper has a MT bench score 6.64. I wonder why the sft model in your Table 4 is 5.94? Will the improvement persist if you start with the 6.64 model?", "reply_to": "1742661587436216615", "quoted": null}, {"id": "1743112676895855040", "text": "We use Zephyr-7B SFT Full  . Additionally, we apply the latest mtbench eval repo.", "reply_to": "1743011648699285829", "quoted": null}, {"id": "1743661872606302659", "text": "Do you generate the samples before training for the whole dataset or you generate them during training?", "reply_to": "1742661587436216615", "quoted": null}, {"id": "1743855505326710821", "text": "At the start of each iteration, generate a set of synthetic examples.", "reply_to": "1743661872606302659", "quoted": null}, {"id": "1747320136082895139", "text": "very interesting work and clever app. of self-play. wonder if there were expts. abt cost-benefit tradeoff of using self-play data vs. synthetic data or  an optimal mix with hybrid strategy ? sounds like a neat active learning sys. design", "reply_to": "1742369496420213182", "quoted": null}, {"id": "1748446088158183716", "text": "why do you think there's no improvement on MMLU? Like it just seems to show a lot of improvement on truthfullQA which i am not sure how it correlates with \"perceivable\" performance when a human uses the models.", "reply_to": "1748442069184729361", "quoted": null}, {"id": "1748454531061710897", "text": "Based on the evaluation from the Hugging Face OpenLLM leaderboard, which employs an older version of Eleuther AI Harness, there is a marginal improvement on MMLU after 4 iterations. However, it's worth noting that Ultrachat200K may not be an ideal SFT dataset for the MMLU task. I'm sure using other SFT datasets could lead to better performance on MMLU. \n\nNote that the vanilla version of SPIN doesn't aim for achieve superhuman AGI; its primary goal is to excel in SFT. The quality of the SFT dataset directly influences SPIN's performance, so a higher-quality dataset translates to better results.", "reply_to": "1748446088158183716", "quoted": null}, {"id": "1748457101712875547", "text": "Personally interested in two questions: 1. Will the self-play improvement become smaller or larger with the model size/capability scaled up? 2. I guess self-play requires a minimum model capability to get improvement. Where is the lower bound of capability (and how to define it)?", "reply_to": "1748442069184729361", "quoted": null}, {"id": "1748460011150606714", "text": "Thank you for the insightful questions. Regarding the first one, we are planning to conduct experiments on 34B and 70B models, aiming to showcase the application of SPIN to larger models. I'm aware that there are people working on that as well. For the second question, it's my personal favorite. In my prior research, we've successfully established over 10 lower bounds in optimization, RL, games, and more. I hope we can resolve this particular lower bound as well (but not in the sense of mathematical proof)\ud83d\ude04.", "reply_to": "1748457101712875547", "quoted": null}, {"id": "1748458319294218568", "text": "And: 2.1. Where is the upper bound of self-play improvement? How is the upper bound affected by the model capability and self-play training data?", "reply_to": "1748457101712875547", "quoted": null}, {"id": "1748517406677381629", "text": "How many iterations have you actually tried? Why stop at 3? Not to say this is what\u2019s going on, but it gives the impression that it either plateaus really hard or even regresses beyond this point. Is this mentioned in the paper?", "reply_to": "1748442069184729361", "quoted": null}, {"id": "1748518889099776367", "text": "Continuing training beyond this point results in very marginal performance gains (but no degeneration), as SPIN has effectively harnessed the potential of the SFT dataset. This is why we stop training at iter 3 (i.e., 4 iterations, with the first one labeled as iter 0)", "reply_to": "1748517406677381629", "quoted": null}, {"id": "1748522831481061707", "text": "But it does plateau, their graph shows as much", "reply_to": "1748517406677381629", "quoted": null}, {"id": "1756018846442688525", "text": "Marked it! Very useful! BTW, did you train the 7B models with global batch size 64 on an 8*A100 80G machine?", "reply_to": "1756014787761483971", "quoted": null}, {"id": "1756053802023899573", "text": "Yes, we use a global batch size of 64 and require 8*A100 80G machine.", "reply_to": "1756018846442688525", "quoted": null}, {"id": "1756040454498648450", "text": "It will be interesting to see how models bias themselves. \n\nSystem prompt, goal setting/benchmarks and a pristine initial dataset probably going to be the most important factors.\n\nMethinks SPIN will be good when paired with a narrow LLM focus (as opposed to generalized).", "reply_to": "1756036806473302209", "quoted": null}, {"id": "1756040644781609229", "text": "For teacher student scenarios i think it will work better than anything else", "reply_to": "1756040454498648450", "quoted": null}, {"id": "1756114115398160554", "text": "Interesting papers, any signs of collapsing?", "reply_to": "1756014787761483971", "quoted": null}, {"id": "1756146761004106047", "text": "No, it\u2019s not GAN, no collapsing issue.", "reply_to": "1756114115398160554", "quoted": null}, {"id": "1756226118816833652", "text": "Really nice-looking work but wtf is going on with the prompt+response pairings in table 5 and 6? These do not match, right?", "reply_to": "1756014787761483971", "quoted": null}, {"id": "1756226811329962412", "text": "The prompt for table 5 is swapped with 6, can you confirm this is just an error in the paper and not in the dataset/dataloaders \ud83d\ude05", "reply_to": "1756226118816833652", "quoted": null}, {"id": "1756352473735065634", "text": "they compare to DPO but they actually start with a DPO'd model. so this doesn't look like a competing method but a complementary one. interested to see if the results hold at larger scales than 7b", "reply_to": "1756036806473302209", "quoted": null}, {"id": "1756363883537932554", "text": "Yes, indeed \ud83e\udd17", "reply_to": "1756352473735065634", "quoted": null}, {"id": "1756371593339732089", "text": "Huge congrats!  I have the following Qs:\n\n(1) Is there any acc / bleu curve w.r.t iteration about \u201cmain player\u201d classifying synthetic or human data / updated Opponent Player generating? This can help investigate the mechanism of SPIT, i.e., how to approach real distribution.", "reply_to": "1756014787761483971", "quoted": null}, {"id": "1781556291149988138", "text": "Thanks for this excellent work. I found you also mentioned the hallucinations of LLMs when using this method to fine-tune LLMs. Can this method totally solve this problem, or the problem is still there\uff1f", "reply_to": "1742369496420213182", "quoted": null}]