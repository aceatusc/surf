[{"id": "1776082272925753811", "text": "You can train on the test set!!", "reply_to": "1776072564244410773", "quoted": null}, {"id": "1776083031616520622", "text": "i'm sure coauthors can explain better but i think the hypothesis is indeed smth like \"instruction data is already in the trainset, u just need to elicit the behavior out of the model, and that's surprisingly parameter-efficient\"", "reply_to": "1776082272925753811", "quoted": null}, {"id": "1776083781486829765", "text": "i don't understand the method yet. doesn't sound like the usual activation PCA. pls share if you extract more info", "reply_to": "1776070594280530022", "quoted": null}, {"id": "1776093209044881571", "text": "It\u2019s definitely a little weird. Like,  because their particular LoReFT method learns parameters that are specific to positions in the prompt. That\u2019s kinda strange", "reply_to": "1776083781486829765", "quoted": null}, {"id": "1776084459483533435", "text": "Pretty wild. How big are the learned parameters? Wondering how it compares to the uncompressed text.", "reply_to": "1776062638205579727", "quoted": null}, {"id": "1776159543409402121", "text": "big enough that it's not doing anything crazy compared to information theoretic limits lol. I think due to precision it's also more bytes than the original text. cc ", "reply_to": "1776084459483533435", "quoted": null}, {"id": "1776268693447799003", "text": "Its 4097 parameters if saved on disk as bfloat16, its 17.5 KB. Something to try is compressing multiple index2long-sentence mappings into a single rank1 intervention! haven\u2019t tried that one. It will be interesting if this can store like a disk lol. We tried index2word in the Appendix.", "reply_to": "1776084459483533435", "quoted": null}, {"id": "1776091444077572283", "text": "thanks   for sharing. \n\nwant to mention that, although ReFT is like a ML technique that hill-climbs, interpretability insight (esp. linear subspace) plays a significant role.\n\ne.g., ReFT subspaces are composable (appendix G).", "reply_to": null, "quoted": "1776079121241497690"}, {"id": "1776102566218338555", "text": "I'm quite interested in the performance of mathematical reasoning, which seems still some way to go. Any possible ways to further improve?", "reply_to": "1776077775180575093", "quoted": null}, {"id": "1776103755412619727", "text": "great q! we are exploring ways to improve math reasonings - things to try are more schematic interventions, e.g., sharing weights across layers, intervening on decoding steps.", "reply_to": "1776102566218338555", "quoted": null}, {"id": "1776162570455687427", "text": "Thank you for reading our work, appreciate the thread :) we will have more details in a thread tomorrow and will talk about some of these points!", "reply_to": "1776156462252761222", "quoted": null}, {"id": "1776163495832347121", "text": "Also dang if your tweet is late then ours is gonna be really late \ud83d\ude33", "reply_to": "1776162570455687427", "quoted": null}, {"id": "1776216392733524254", "text": "ReFT trains for 6 epochs, but DoRA/LoRA trains for 3 epochs. \nSeems not a fair comparison \ud83e\udd14", "reply_to": "1776057023697731913", "quoted": null}, {"id": "1776232857805177275", "text": "that does not matter for the performance gain, \nif you try LoRA for more than 3 epochs it overfits. \nWhereas there is potential that ReFT would be easier to generalize and overfit at the same time", "reply_to": "1776216392733524254", "quoted": null}, {"id": "1776389254601654390", "text": "hey! DoRA is pretty cool work i think - for hyperparameters, it might be even better if DoRA can try the same hyperparameter selection on GSM8K train last 300 (can hillclimb as hard as you can since there is no test leakage, and we did that), and see whatever the number is? maybe also lower the training epoch?\n\nReFT is also kinda different, so we have to do this... so many unknowns.", "reply_to": "1776216392733524254", "quoted": null}, {"id": "1776273357991772550", "text": "Would be interesting to test with the same epochs. The repo is available.", "reply_to": "1776216392733524254", "quoted": null}, {"id": "1776540825016774846", "text": "sry to keep going back to this - I always realized there was a typo in our appendix table (title of 5 and 6 should be swapped); but one additional thing to add, batch size for LoReFT is 32, so step # are the same!", "reply_to": "1776216392733524254", "quoted": null}, {"id": "1776216532529672344", "text": "ReFT trains for 6 epochs, but DoRA/LoRA trains for 3 epochs.\nSeems not a fair comparison \ud83e\udd14", "reply_to": "1776156462252761222", "quoted": null}, {"id": "1776220958535118941", "text": "They tune epochs per-task, and use a method's suggested number of epochs. Not sure where you got this from, but for example LoRA also heavily tunes that per-task and (seems to?) select on the metric that they report.", "reply_to": "1776216532529672344", "quoted": null}, {"id": "1776341370841780498", "text": "while playing w/ ReFT, we're concerned about neuron- (or circuit-) level interp. \n\nwith a very limited linear subspace, we can steer models to perform many tasks. \n\nWhat does it imply, then, when we assign a set of functional words to neurons? esp. in a 1-to-1 mapping manner?", "reply_to": null, "quoted": "1776331947566047410"}, {"id": "1776349843037769814", "text": "if you are doing interventions is it possible to do reft for representations and lora for weights at the same time? would there be any benefit to that?", "reply_to": "1776331947566047410", "quoted": null}, {"id": "1776362721363304921", "text": "yes, possible in theory! we haven't tried it yet so unclear if it would work in practice", "reply_to": "1776349843037769814", "quoted": null}, {"id": "1776377206492815757", "text": "What do you mean by \"representations\"? Features? Sorry for the dumb question :)", "reply_to": "1776331947566047410", "quoted": null}, {"id": "1776377772707123705", "text": "so the input representations are just the embeddings of the input tokens. then, each transformer layer takes in the most recent representations, does some operations (attn, MLP) on the whole sequence, and spits out updated representations.\n\neach layer's output is what we mean", "reply_to": "1776377206492815757", "quoted": null}, {"id": "1776413586522317145", "text": "Great paper! Question: If I do lora (W = W + AB) on a single layer's weight applied to certain selected tokens at forward pass, would it be equivalent to doing ReFT with phi(h) = h + W^{-1}ABh? (at forward pass Wphi(h) = Wh + ABh = (W+AB)h)", "reply_to": "1776331952414736541", "quoted": null}, {"id": "1776459713477312684", "text": "great q! offline, we tried another variant, Noreft:\n\n \n\niiuc, Noreft is similar to your proposal? Noreft is not as good as Loreft so far. \n\ntho i hope pyreft makes it easy to define your own intervention and try it out! Tuning can be quick.", "reply_to": "1776413586522317145", "quoted": null}, {"id": "1776493910337995251", "text": "This seems basically the same as parameterizing the intervention as a low-rank matrix plus a skip connection; does this parameterization help somehow?\n\nI tried such a low-rank intervention as a latent space adversary for an LAT project, but it learnt poorly, or my task was weird.", "reply_to": "1776331956017676763", "quoted": null}, {"id": "1776888901489369250", "text": "I think that's not quite the same because our intervention is a fxn of (residual stream + layer output), whereas what you state has the intervention be a fxn of only layer output? (Correct if wrong.) Ours should be more expressive then, it has access to the entire state", "reply_to": "1776493910337995251", "quoted": null}, {"id": "1776582058048909577", "text": "Is there a particular reason why there is no comparison done with VERA or the DoRA hybrid DVoRA?\n\nDVoRA in theory should offer low parameters but good performance?", "reply_to": "1776331947566047410", "quoted": null}, {"id": "1776885531244392485", "text": "Thanks for bringing VeRA to our attention! \n(1) We need to verify if hparam tuning was done on dev before adding more methods to our tables. We want fair comparisons.\n(2) We're interp people so not familiar with PEFTs and have had to read a lot of papers in recent weeks \ud83d\ude05", "reply_to": "1776582058048909577", "quoted": null}, {"id": "1776628884387012608", "text": "this is fantastic. running it on my cuda machine and it's a blast. took a quick look at dependencies and looks like aside from flash attn, no custom kernels? wondering how hard it would be to port over to apple mlx? ( ) and ( )", "reply_to": "1776331979304452274", "quoted": null}, {"id": "1776884824923636106", "text": "Thanks for interest! The major dependency is pyvene which handles bespoke torch hooks necessary for interventions on the forward pass. Porting (a barebones version of) that to MLX will be a big project IMO, idk about hook support there. But otherwise no hardware optims done.", "reply_to": "1776628884387012608", "quoted": null}, {"id": "1776662547908735359", "text": "Hey, I read through the paper. Amazing work! I am curious how much efforts are required to be devoted into hyperparameter search (in L, r, p+s) to make loreft work efficiently? How many times of training FLOPs would be required here? Will method like AutoPEFT help? Thanks a lot!", "reply_to": "1776331947566047410", "quoted": null}, {"id": "1776883796941640179", "text": "We give some best practices for hparam search in appendix D. In general I would start with all layers, p5+s5, rank 8. If perf is bad, double p/s (little added training cost) or double rank (slows down training). If perf is good, halve rank repeatedly + cut layers to save params.", "reply_to": "1776662547908735359", "quoted": null}, {"id": "1777040969189278005", "text": "Really well done experiments! I was thinking about something similar but without the linear subspace projection. Do you have an intuition whether downprojecting to a subspace is necessary for quality or is it mostly a tool to gain parameter efficiency?", "reply_to": "1776331947566047410", "quoted": null}, {"id": "1777244940025938167", "text": "Two things:\n(1) increasing rank of the subspace too much reduces perf (when holding training data constant). probably increasing the subspace size slows convergence by complicating the optimisation?\n(2) removing orthog. on R also reduces perf\nSo it seems important for quality!", "reply_to": "1777040969189278005", "quoted": null}, {"id": "1839280393595314545", "text": "So would linear representation mean that the idea of a preposition, for example, is encoded in a linear subspace? Could you use this idea to look at lexical and phrasal categories?", "reply_to": "1776331954008604934", "quoted": null}]