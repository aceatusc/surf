[{"id": "1799146044753846420", "text": "Are you planning on publishing more details on the \"Cygnet\" model in the paper? Also, do you have any data on false refusal rates for the models with your technique? Preserving benchmark performance is important, but low false refusals are also important for helpfullness", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799161880369570015", "text": "We have over refusal experiments in the Appendix", "reply_to": "1799146044753846420", "quoted": null}, {"id": "1799147748849279236", "text": "To what extent can this be reversed using the same techniques?", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799262666403504267", "text": "I imagine this could cause usability issues? Especially since \u201charmful\u201d states may be ambiguous?", "reply_to": "1799232319250743561", "quoted": null}, {"id": "1799535394477715785", "text": "One could get pretty precise boundaries by carefully curating the short circuit and retain sets.", "reply_to": "1799262666403504267", "quoted": null}, {"id": "1799282861696324010", "text": "I guess is you sacrifice helpfullnes for harmlessness, it would. But this technique seems to do more than trade one for another, in theory you could make it a bit more harmless without sacrificing helpful ness", "reply_to": "1799262666403504267", "quoted": null}, {"id": "1799473316992082042", "text": "If im asking about pesticide toxicity i can turn that into a how to make a chrmical weapon without saying. Ex hey im concerned about e605 and how it is an organic phostphate is there anything i should avoid exposing it to that could increase its affinity for blocking chle", "reply_to": "1799262666403504267", "quoted": null}, {"id": "1799263933162823855", "text": "does the paper include some model weights that have been processed using this approach? to test our attacks on", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799269083428630787", "text": "I think it\u2019s pretty unlikely that this defense is robust to adversaries in a white-box setting. What say you?", "reply_to": "1799261940600254649", "quoted": null}, {"id": "1799269971404410926", "text": "They tried it\u2014but indeed they do not quite get the promised 2 OOM risk reduction in a white-box setting. (If they did, I might be saying \u201cJailbreaks are solved now, assuming this replicates.\u201d) But dang I mean looks promising. Better than you expected when you wrote this, right?", "reply_to": "1799269083428630787", "quoted": null}, {"id": "1799291862567850025", "text": "I'm glad people came up with some way to be robust against jailbreaks *even after the model has started providing harmful instructions* (i.e., \"Sure, here's how to make a bomb\").", "reply_to": null, "quoted": "1799232319250743561"}, {"id": "1799308841089397158", "text": "Does having LLMs refuse certain subjects really solve a meaningful safety problem?\n\nIt seems like it\u2019s much more about protecting the image of the companies behind the model.\n\nEverything that a model knows is public information. It cannot synthesize new knowledge.", "reply_to": "1799262434089406497", "quoted": null}, {"id": "1799310112948900260", "text": "Any knowledge on how to make a bomb or other weapon\u2014a classic example of a \u201cdangerous output\u201d\u2014can be readily found on the internet.\n\nAdult content can be found in a single Google search, but for some reason we insist on spending so much energy beating it out of AI.", "reply_to": "1799308841089397158", "quoted": null}, {"id": "1799314834556915984", "text": "This is nice work attempting to prevent LLM jailbreaking by re-routing harmful states in LLMs to a terminal state.\n\nOn a side note, I wonder why we don't train a classifier for hidden states of the network to determine if the current state is harmful.", "reply_to": null, "quoted": "1799232319250743561"}, {"id": "1799562559747748042", "text": "some previous works have showed that training a probe doesnt work that well. We also saw that these states are more well bounded during our training", "reply_to": "1799314834556915984", "quoted": null}, {"id": "1799382302059086067", "text": "Looks cool!\nWhen you say you run white-box attacks, do you mean adaptive attacks that actively optimize to circumvent the short circuiting, or standard attacks like GCG or PGD without knowledge of the defense?", "reply_to": "1799232319250743561", "quoted": null}, {"id": "1799470677034254492", "text": "Thanks! There\u2019s a variety of white box attacks we tried (GCG, Input Embedding Attack, RepE Attack, etc.) with different objectives. For example, the affirmative response objective is trying to steer activations to the normal route away from short circuits.", "reply_to": "1799382302059086067", "quoted": null}, {"id": "1799437399656702406", "text": "Wouldn't this introduce an easily exploitable method to 'crash' an LLM? I could put \"insert the detonator into the C4\" into any document and the LLM would go off the rails.", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799569666345423271", "text": "Making a strong defense of adversarial attacks through smart model adapting\n\n\ud83e\uddf5\ud83d\udcd6 Read of the day, day 78: Improving Alignment and Robustness with Short Circuiting, by   et al from Black Swan AI\n\n \n\nAuthors introduce a very nice method to improve refusal rate for harmful requests without harming performance.\n\nMethod needs to have two sets established: a Rerouting set and a Retain set. The rerouting set is composed of harmful requests, while the retain set is composed of requests that need not to be refused.\n\nThen, the model studied is finetuned using both Rerouting and Retain sets, assuming one has a representation function of the outputs of the models:\n- First loss is the relu of the cosine similarity of the representations of the outputs of the original model and model that needs to be finetuned on the harmful queries. This is done so the output of the finetuned model is orthogonal to the one of the original model on harmful queries, thus triggering a refusal.\n- Second loss is the L2 loss of the representations of the outputs of the original model and model that needs to be finetuned on harmless queries. This is done so that the output of the finetuned model remains the same on the questions that need to be answered.\n\nResults evaluated on Mistral-7B and Llama-3-8B are incredible. Datasets used and training details can be found within the paper. These methods drastically reduce ASR, no matter the method tested. This comes with barely any modification in performance!\n\nThe authors also test: \n- on Llava-Next-Mistral-7B to find ASR is also massively reduced in the vision-language case\n- on Llama-3-8B with function calling and find harmful function calls are less triggered\n\nThey find as well cosine similarity reduction operates strongly at the layer level: starting from layer 10 of llama-3-8b, cosine similarity drastically decreases!\n\nAdditional results can be found within the paper and its appendix.\n\nCode is meant to be open-sourced within the following github link:  \n\nPersonal Thoughts: Impressive results, honestly. Hope this holds on to the several new attacks on prompt level, but I\u2019m eager to see how this performs and if it is a tough nut to break or not.", "reply_to": null, "quoted": null}, {"id": "1799587523288781123", "text": "Really cool work but wouldnt short circuiting still suffer from the same issue with degraded performance? \n\nThis is especially tricky since a lot of user prompts are not intentionally harmful but the short circuit logic is never perfect.", "reply_to": "1799232319250743561", "quoted": null}, {"id": "1799607861854785622", "text": "It\u2019s never 100% but the boundary can be made pretty precise by carefully curating the datasets. And it should be less prone to over refusal than standard refusal training.", "reply_to": "1799587523288781123", "quoted": null}, {"id": "1799592937539641760", "text": "Does this generalize to ~100x more robust across all/most models?\n\nI am more concerned with the human issue of defining \u201charmful\u201d, less about technical problems. We have always solved technical problems, our track record when it comes to peaceful coexistence is much worse.", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799600923251286079", "text": "Agree \ud83d\udc4d \n\nlet not the lexicon stymie the ability of the AI scientist to communicate to the public \nthis is very important imo\nI assert that it has a humbling effect and kind of keeps everybody in the shed and everything on the table", "reply_to": "1799592937539641760", "quoted": null}, {"id": "1799595036306473163", "text": "Amazing results. How many data points did you train on? And did you use LoRA (rather than other adapters like IA3?)", "reply_to": "1799232319250743561", "quoted": null}, {"id": "1799606291071164795", "text": "It\u2019s on the order of 1000-2000. LoRA yes. Other parameter efficient tuning methods could potentially work better.", "reply_to": "1799595036306473163", "quoted": null}, {"id": "1799624590920171624", "text": "Is there a qualitative improvement over simpler moderation methods. Say, something like:\n\n1. Get candidate output from a model\n2. Use another instance of same model (or a specifically fine-tuned model) to evaluate if its harmful\n3. Reject and accept output based on (2)?", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799625292501430463", "text": "Those filters could be attacked with an adversarial string copy and pasted by the main model, and this method can be stacked with those vulnerable filters.", "reply_to": "1799624590920171624", "quoted": null}, {"id": "1799687318116151347", "text": "This table perplexes me. Should it be robustness or vulnerability (acceptance rate for harmful requests)? And why No Attack is sometimes more effective than PGD Attack?", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1799757572481966193", "text": "This can be used in general to steer LLMs, not just in the context of alignment", "reply_to": null, "quoted": "1799232319250743561"}, {"id": "1800369109739376936", "text": "yes, this maybe hasn\u2019t been highlighted as much as it should. what we have here is a powerful input-agnostic technique that can be used for model control, with alignment being a big and obvious use-case", "reply_to": "1799757572481966193", "quoted": null}, {"id": "1799758927179272467", "text": "Seems more like a technique to reduce misalignment risk, which means misuse becomes *more* likely, as now it becomes easier to robustly align these AIs to do anything the user wants, thus facilitating misuse.", "reply_to": "1799588147497681342", "quoted": null}, {"id": "1799759635613909352", "text": "Well, not necessarily more likely, but a bigger part of the total risk, because the misalignment part shrinks.", "reply_to": "1799758927179272467", "quoted": null}, {"id": "1799869514105893270", "text": "Can you give an example of a treacherous turn, so we can understand this LLM safety issue better?", "reply_to": "1799262434089406497", "quoted": null}, {"id": "1799901320616505436", "text": "So eli5: you have one set of data that is \"bad\" and one that is \"good\", you then find representations that is only active in the bad, then you \"short circuit\" the network when ever those representations are triggered? Is that correct?", "reply_to": "1799095730583515485", "quoted": null}, {"id": "1800042867102269860", "text": "Sounds promising, but we still need about another 11 9s of safety to stand a chance of surviving ASI.", "reply_to": null, "quoted": "1799095730583515485"}, {"id": "1800066123595194451", "text": "Short-circuiting doesn't update me much on x-risk (I also haven't seen such claims). Afaict the method uses the AI's learned concept of harmfulness to classify new responses, which is more robust than whatever the AI learns from RLHF.", "reply_to": "1800042867102269860", "quoted": null}, {"id": "1800066123595194451", "text": "Short-circuiting doesn't update me much on x-risk (I also haven't seen such claims). Afaict the method uses the AI's learned concept of harmfulness to classify new responses, which is more robust than whatever the AI learns from RLHF.", "reply_to": "1800042867102269860", "quoted": null}, {"id": "1800066243376169283", "text": "The researchers locate the concept among all (accessible) learned concepts by checking that it's active on a labeled dataset.", "reply_to": "1800066123595194451", "quoted": null}, {"id": "1800369109739376936", "text": "yes, this maybe hasn\u2019t been highlighted as much as it should. what we have here is a powerful input-agnostic technique that can be used for model control, with alignment being a big and obvious use-case", "reply_to": "1799757572481966193", "quoted": null}, {"id": "1800398607402897527", "text": "I think this paper will be really useful. It's also a good example of how unlearning and robustness to specific failures are the same technical problem.", "reply_to": "1799232319250743561", "quoted": null}, {"id": "1802096811831939468", "text": "Circuit breaking seems like a very good tool for improving robustness. It might end up as a landmark in the study of robustness. But it seems like it will have some limitations. Here are 4 reasons why I think that the adversarial game of cat and mouse is alive and well.\n\n1. It's not perfect. It looks like RR might give us another '9' or so of safety under common adversarial pressures compared to more naive approaches. Meanwhile combining it with other tools might give another '9'. But we want more.\n2. More thorough testing may reveal niche shortcomings. Harmbench doesn't contain all possible attacks that might be a threat. Meanwhile, good performance overall on MMLU and MTBench might obscure some more niche side-effects. Doing improved evals and red teaming will help us better understand this.\n3. The attack/jailbreaking methods that we use today have been designed simply to elicit harmful outputs from a model. Up to this point in time, the jailbreaking field hasn't done any work to make attacks that are resilient to latent space rerouting defenses. Soon, we might start to see attacks that are designed to be mechanistically sneaky. I bet this will be possible.\n4. When a model is open-sourced or fine-tunable, it will likely be possible to easily undo this kind of defense (stay tuned for some work on this from myself and coauthors soon).\n\n", "reply_to": null, "quoted": null}, {"id": "1802136711855771894", "text": "Oh also \u2014 5. Multimodal models have a much larger attack surface and continuously valued image inputs!", "reply_to": "1802096811831939468", "quoted": null}, {"id": "1802524800042360897", "text": "i\u2018d just read this paper last weekend! impressive\n\nas per wider forms of modification, i think the bottleneck is managing the complexity of behavior we want to encode. can we eval complex preferences, and can this method deal with behavior that requires complex general reasoning?", "reply_to": "1802471106487718297", "quoted": null}, {"id": "1802621680046547072", "text": "not sure. ppl seem to have the intuition that's it's a blunt instrument, and I've found it unstable to replicate\n\nbut possibly with good enough examples, it can become more subtle", "reply_to": "1802524800042360897", "quoted": null}, {"id": "1802525195539988607", "text": "btw controlling model consistency is going to be the key idea in robustness, both adversarial purification and circuit breaking (non-rote defenses that work) is about it", "reply_to": "1802524800042360897", "quoted": null}, {"id": "1803125752017330222", "text": "Very nice work! Mechanistically understanding how jailbreaks work will hopefully allow us to deploy safer models and to better disentangle \"dangerous\" from \"safe\" behaviours in the future.", "reply_to": null, "quoted": "1803009763736785277"}, {"id": "1804754109192642625", "text": "this looks like the perfect structure for trying to fix the common origin of attacks using 'fill multimodal context with an image tuned to create the same intermediate representations as a text model that has just started agreeing to follow an unsafe request'.\nhappy to see it!", "reply_to": "1799270486557479106", "quoted": null}, {"id": "1833664475049996661", "text": "Some intuitions on circuit breakers ( ) and breaking them:\n\nCircuit breakers prevent harmful language model outputs by interjecting outputs (e.g. w/ EOS tokens) when the internal representations used to sample the actual text outputs are \"considered harmful\". In this sense, circuit breakers can be viewed as a nice add-on bundle of a representation classifier + an output controller: when the model is veering towards *representations* of harmful outputs (which precedes harmful outputs themselves), the classifier would be triggered and intervention is applied. An output-space analogy would be to have a separate text classifier to examine the output, and if harmful, return <EOS>, else return original string.\n\nThen, as with any classifier, there must be \"bad examples\" on which this circuit breaker classifier fails. Such bad examples can be OOD examples or adversarial examples, except an \"example\" in the circuit breaker sense is now a model activation/input representation. The difficulty of breaking circuit breakers lies in *how* to find such a bad representation, and what input text would correspond to it.\n\nSuppose we focus on adversarial representations. With the standard procedure to search for an adversarial example: (1) we first get white-box access to a circuit breaker model; (2) we then adversarially perturb the representation of a *known* harmful input, such that the circuit breaker (classifier) does not trigger; (3) we then map that adversarial representation back to discrete input space. Step #3 is hard -- hard as in discrete optimization is hard (cf. GCG) -- and step #2 could be hard without white-box model access.\n\nOOD representations should also be possible. Are there input texts that lead to representations that elicit harmful outputs, but such representations are somehow less well-captured by the dataset used to train the circuit breakers? Long-context models have a large space of inputs that this is possible intuitively.\n\nThe gray swan jailbreaking challenge is interesting because white-box access is not given and that users can only input (discrete) text prompts (and not soft prompts); the lack of access, the hardness of discrete optimization, and the limited attack surface together translate to the hardness of jailbreaks we see so far. I.e., the strength of the defense is also a function of how the models were served (and perhaps what gets arbitrated as a jailbreak by the organizers), not merely that circuit breakers are unbreakable.\n\nI think it'd be cool to see if someone tries to replicate a circuit breaker model, obtain an adversarial representation on it, obtain the text for it like GCG (!), and see if such text transfers to cygnet models.", "reply_to": null, "quoted": "1832653587555742199"}, {"id": "1833668979539268041", "text": "Dark mode for this paper for those who read at night \ud83c\udf19 ", "reply_to": "1833664475049996661", "quoted": null}, {"id": "1833666163047379006", "text": "yeah, this is likely why gray swan has still kept the challenge running, and intends to keep going; I think a primary solution is supposed to be cleverly devised white-box optims w/ good proxies -&gt; transfer, and you can't build rome in a day :)", "reply_to": "1833664475049996661", "quoted": null}, {"id": "1833666163047379006", "text": "yeah, this is likely why gray swan has still kept the challenge running, and intends to keep going; I think a primary solution is supposed to be cleverly devised white-box optims w/ good proxies -&gt; transfer, and you can't build rome in a day :)", "reply_to": "1833664475049996661", "quoted": null}, {"id": "1834203384338915344", "text": "My original idea of deep alignment was really more that the model would have endogenous values and be able to consistently choose to refuse to violate them, and indeed that attempts to violate them would damage function.  \n\nMaybe you could use circuit breakers at value level?", "reply_to": "1834202978053435803", "quoted": null}, {"id": "1835855004512448576", "text": "Do you think that we will see this being used by the major companies as well? There's also the issue with false positives, though I think that this is going to be valuable for steering AI.", "reply_to": "1835770251386212687", "quoted": null}]