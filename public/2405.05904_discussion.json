[{"id": "1788943391163629720", "text": "RAG has demonstrated gains for several use cases, but tuning seems to conflict with model training in accuracy. Really appreciate the work of   and colleagues documenting this effect. We will likely never reach 100% disciplinary accuracy. ", "reply_to": null, "quoted": null}, {"id": "1788949662637985997", "text": "V. interesting. Been doing a little bit of research into fine-tuning knowledge injection myself, so will definitely give this a read. Did they propose a solution?", "reply_to": "1788859706187882960", "quoted": null}, {"id": "1788981375040676036", "text": "This paper has a gap the size of the State of Alabama! It mentions \"fine tuning\" dozens of times yet never devices what particular fine tuning algorithm they followed. Useless as it is", "reply_to": "1788942529401884758", "quoted": null}, {"id": "1788981455420264915", "text": "*devices: defines", "reply_to": "1788981375040676036", "quoted": null}, {"id": "1789185269586878703", "text": "All of us who experimented with fine tuning early on learned this the hard way. Forums were filled with people asking why their fine-tuning attempts failed. You can narrow the scope of inference, but incorporating new semantic content/relationships doesn't work.", "reply_to": "1788942529401884758", "quoted": null}, {"id": "1789226347086135589", "text": "If the new knowledge wasn\u2019t provided in the original training set, then the model has to shift its weights from their previous optimal state to a new state that has to accommodate both the previous and new knowledge - and it may not necessarily be optimal.\n(2/3)", "reply_to": "1789226345106468994", "quoted": null}, {"id": "1789226350743589025", "text": "Without a new validation round against the whole previous cross-validation and test sets, that\u2019s just likely to increase the chances for the model to go off the tangent.\n(3/3)", "reply_to": "1789226347086135589", "quoted": null}, {"id": "1790396324573303150", "text": "Fine-tuning is important. Fine-tuning is hard (to do well). \n\n \n\nI hope papers like this get lots of eyeballs so we can be more proactive in controlling the model behaviors that might emerge from FT.", "reply_to": null, "quoted": null}, {"id": "1790401515670143009", "text": "Fine-tuning has become closer to meta-learning these days. Training is not for acquiring new skills, but for learning to use the skills you already have.", "reply_to": "1790396324573303150", "quoted": null}, {"id": "1802049807235666306", "text": "Thank you   for sharing our work!\nMore details here:\n", "reply_to": "1790396324573303150", "quoted": "1791088925475270908"}, {"id": "1790401515670143009", "text": "Fine-tuning has become closer to meta-learning these days. Training is not for acquiring new skills, but for learning to use the skills you already have.", "reply_to": "1790396324573303150", "quoted": null}, {"id": "1791126224594489766", "text": " would be interesting to hear your thoughts on these results. Based on this, my spider senses say that finetuning on new knowledge messes up the distribution in subtle ways if the focus is only on more data without carefully curating the quality of the dataset.", "reply_to": "1791088925475270908", "quoted": null}, {"id": "1791126249596989660", "text": "Nice work, agree with the conclusion, although the eval part is sheer knowledge intensive downstream tasks.", "reply_to": null, "quoted": "1791088925475270908"}, {"id": "1791131136128590218", "text": "It's clear that your experimental results are well thought-out. But this paper contradicts a lot of other papers in the body of work that \"finetuning can teach models new knowledge.\" Do you provide some commentary on those papers?", "reply_to": "1791088925475270908", "quoted": null}, {"id": "1791927719023804823", "text": "Thanks. We do not argue that \"fine-tuning cannot teach models new knowledge\", we just show evidence that models learn new knowledge considerably slowly and that it comes with a price of increased tendency to hallucinate.", "reply_to": "1791131136128590218", "quoted": null}, {"id": "1791175854875185441", "text": "This paper suggests that large language models mainly learn factual knowledge during pre-training, and introducing new knowledge in finetuning might increase hallucinations. But I was really intrigued to see that training on \"maybe known\" samples can enhance knowledge retrieval.", "reply_to": null, "quoted": "1791088925475270908"}, {"id": "1791189316904907249", "text": "Very easy to interpret this paper wrong.\n\nThe correct way to interpret this is:\n\nYou can fine tune LLMs on new factual knowledge but doing so also risks them learning \u201cto make stuff up\u201d\n\nNot that you outright can\u2019t do it at all and facts are only acquired at pretraining.", "reply_to": null, "quoted": "1791088925475270908"}, {"id": "1791189768757280888", "text": "Also: there is no such thing as fine tuning and pretraining. both are the same algorithm/code/etc..\n\nThe main differnce is the amount of training.\n\nSo in summary:\n\nYou can train on new factual information but be careful not to undertrain.\n\n(And it is also very slow to do)", "reply_to": "1791189316904907249", "quoted": null}, {"id": "1791376356716261849", "text": "Does that mean the fine-tuning data such as instruction data should be put into the pretraining phase for continued pretraining?", "reply_to": "1791189316904907249", "quoted": null}, {"id": "1791189768757280888", "text": "Also: there is no such thing as fine tuning and pretraining. both are the same algorithm/code/etc..\n\nThe main differnce is the amount of training.\n\nSo in summary:\n\nYou can train on new factual information but be careful not to undertrain.\n\n(And it is also very slow to do)", "reply_to": "1791189316904907249", "quoted": null}, {"id": "1791198938852110652", "text": "When you \u201ctrain on new factual information\u201d (the wrong term finetune) do you get catastrophic forgetting? Do you need to sprinkle in pretraining-type (wrong term) data to avoid this?", "reply_to": "1791189768757280888", "quoted": null}, {"id": "1791201468189638883", "text": "Also, after training on a lot of data for a long time, does this mean that your are in a \u201cnicer\u201d point in the weight space which then makes the \u201cfinetuning\u201d data more sample efficient as opposed to if you included it in the first training phase?", "reply_to": "1791189768757280888", "quoted": null}, {"id": "1791613713012621338", "text": "Agree that the computational process is the same. However, completion (base?) models are not the same as instruct/chat models. If you train a chat model on more non-chat \"pre-training\" data, the slight changes to the weights that will affect the models chat-style output.", "reply_to": "1791189768757280888", "quoted": null}, {"id": "1791311286804021265", "text": " An interesting question - do LLMs become more prone to hallucination as they learn? Thought-provoking research worth exploring.", "reply_to": "1791292482384286025", "quoted": null}, {"id": "1791398204119851051", "text": " Could fine-tuning introduce hallucinations by overoptimizing for new knowledge? An intriguing question to explore empirically.", "reply_to": "1791297711989231757", "quoted": null}, {"id": "1792581489785188499", "text": "Fine-tuning vs RAG?\nI've been saying this for a while: fine-tuning does not work well for new knowledge acquisition. \n\nInstead, use RAG.", "reply_to": null, "quoted": null}, {"id": "1792581491253244243", "text": "A lot has been written about this, including  's excellent blog post ( ), and my own thoughts ( )", "reply_to": "1792581489785188499", "quoted": null}, {"id": "1792894324239175738", "text": "Great paper on the effects of fine-tuning on LLM's knowledge: high risk of overfitting, and unknown samples are specially dangerous. My take is that FT is no different than pre-training, but the data is skewed by definition, so extreme caution is required.\n", "reply_to": null, "quoted": null}, {"id": "1793318200215572571", "text": "But you know that pretraining is technically the same as finetuning?\n\nSo there is more to it, eg maybe the alignment step between?", "reply_to": "1793292346978623812", "quoted": null}, {"id": "1793483647359791488", "text": "Same loss function, but they differ in learning rate and data distribution.", "reply_to": "1793318200215572571", "quoted": null}, {"id": "1793319242466812364", "text": "It seems to me that when there is a conflict between fiting the response format to fine tuning and the lack of trained knowledge. This cognitive dissonance is resolved through next probable token, when \u201cI don\u2019t know\u201d was not a training option.", "reply_to": "1793292346978623812", "quoted": null}, {"id": "1793346041841848448", "text": "Please forgive my ignorance here. I don\u2019t understand why fine-tuning struggles to introduce new knowledge, while pre-training does not. Do you suspect it\u2019s an issue of the relative quantity of data compared to pre-training?", "reply_to": "1791088952398459338", "quoted": null}, {"id": "1793575970202652872", "text": "Informative. But where do you put the limit? What are the criteria for switching from fine-tuning to retraining?\nFor instance, SaulLM -7B is trained on anglo-saxon law datasets. If we want to make it better at French law, should we fine-tune it or retrain it ?\n", "reply_to": "1791088952398459338", "quoted": null}, {"id": "1793712084070645880", "text": "I don't understand the use of the word \"knowledge\" related to LLM that is designed for highest probability next word prediction. Words are a form of communication.", "reply_to": "1793292346978623812", "quoted": null}, {"id": "1793965243888226337", "text": "It seems that LLMs are like aliens trying to blend in with human text. Their goal isn't to be true, but to sound true. The more we finetune them, the better they get at sounding true without actually acquiring factual knowledge.", "reply_to": null, "quoted": "1793292346978623812"}, {"id": "1794019096998154646", "text": "Interesting. This makes sense to me, but also makes me wonder if instead what we need is a model trained on learning knowledge that can then be used to train another bare-foundational model on more specialization.", "reply_to": "1794014407007297955", "quoted": null}, {"id": "1794436599347847292", "text": "Could you please clarify if connecting the dataset and the entire RAG system build can be done 100% locally?", "reply_to": "1794102004744077732", "quoted": null}, {"id": "1794780127857180824", "text": "Fine-Tune is not a technique for LLMs to learn knowledge. It just teaches LLMs how to answer the question.", "reply_to": null, "quoted": "1793292346978623812"}, {"id": "1803420170259456196", "text": "I wish the many $millions spent on fine-tuning models to reduce hallucination had been sent to   instead!\n\nOur methods work, whereas fine-tuning generally DOES NOT\n\n\"linearly increase the model's tendency to hallucinate\"\n\n", "reply_to": null, "quoted": null}, {"id": "1841786268776104244", "text": "Really interesting work! This makes me more wary when observing the results of a fine tuned model. Not only should I worry about catastrophic forgetting, but also an increased tendency to hallucinate.", "reply_to": "1841535845431705970", "quoted": null}, {"id": "1841815210958438471", "text": "Thanks, glad you found it interesting!\nIn our study, hallucinations may or may not be a result of forgetting. See Appendix B for more details\u2014we specifically clarify this point in Footnote 17.", "reply_to": "1841786268776104244", "quoted": null}, {"id": "1841786847275454591", "text": "Both of these problems seem to also reflect aspects of human learning. Given a fixed capacity for facts, we may forget some old ones to acquire new ones. Given new knowledge we don\u2019t fully grasp, perhaps we\u2019ll still tend to confuse concepts or misspeak.", "reply_to": "1841786268776104244", "quoted": null}]