[{"id": "1794057403450654739", "text": "We train and evaluate extensively with various offline preference optimization algorithms, including DPO, KTO, ORPO, RDPO, and more. Hyperparameter tuning significantly impacts algorithm effectiveness.\n\nDPO performs consistently well, but SimPO is better!", "reply_to": null, "quoted": "1794055094389948546"}, {"id": "1794615879193481500", "text": "Congrats on your work. I applied our model extrapolation (ExPO) method to your released checkpoint, hopefully resulting in a better-performing LLaMA-3 model. You can check out the ExPO-enhanced model\ud83d\udc47\ud83c\udffb\n ", "reply_to": "1794057403450654739", "quoted": null}, {"id": "1794142575772709157", "text": "Glad this SimPO paper is finally out. I am intrigued by its simplicity and effectiveness. The team has done a very impressive job in various experimental settings (and careful hyper-parameter tuning!) and in-depth analysis. Kudos to   ", "reply_to": null, "quoted": "1794055094389948546"}, {"id": "1794615678168904172", "text": "Congrats on your work. I applied our model extrapolation (ExPO) method to your released checkpoint, hopefully resulting in a better-performing LLaMA-3 model. You can check out the ExPO-enhanced model\ud83d\udc47\ud83c\udffb\n ", "reply_to": "1794142575772709157", "quoted": null}, {"id": "1794307707463430341", "text": "So this completely ignores the reference model? Sorry if this is discussed in the paper because I haven\u2019t had the chance to read it yet, but what happens if the model diverges (i.e. large policy updates)? I thought the reference model played an important role in policy algorithms", "reply_to": "1794055097212707207", "quoted": null}, {"id": "1794331020051013705", "text": "I think the comparison with dpo means that this still needs a separate sft step before. Could insight from this be used to improve orpo? Simorpo", "reply_to": "1794055094389948546", "quoted": null}, {"id": "1794346250130850022", "text": "remove length norm and gamma its dpo loss without ref model,\n\n  senpai any intuition why is work, \nit should face same catastofic forgeting are sft right?", "reply_to": "1794055097212707207", "quoted": null}, {"id": "1796247911170449416", "text": "Sorry for the delay    \n\nCan't say for sure, but I would def believe that since we're doing away with the constraint to the SFT model, we'd overfit more to the 'task' in the pref dataset\n\nWould be curious to see evals of SimPO models on non-chat benchmarks   :)", "reply_to": "1794346250130850022", "quoted": null}, {"id": "1794389207814996037", "text": "Fantastic paper from   team. Chatting with the model is also incredible.  Llama3 8B is the best small model currently ---SimPO makes it better. Excels on my list of open-domain queries.", "reply_to": null, "quoted": "1794142575772709157"}, {"id": "1794691501488042378", "text": "All for simpler &amp; better RLHF but\n&gt; Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2\u2014surpassing Claude 3 Opus\n\n\u2013 when I see this I think \"that would be too powerful. It definitely overfits to *something*\"", "reply_to": null, "quoted": "1794055094389948546"}, {"id": "1794745201657012515", "text": "Outperforming Claude 3 on AE 2 definitely won't mean our model is stronger than it in all aspects (actually it's worse on Arena-Hard than Claude 3). And overfitting probably exists b/c UltraFeedback labels are generated from GPT-4 and the judge of AE 2 is GPT-4 Turbo.", "reply_to": "1794691501488042378", "quoted": null}, {"id": "1794718003164287068", "text": "Does having no reference for the reward function mean that we don't have one specific way to align the model?\n\nWe're replacing a policy based on y|x with the average log proba which may have the effect of smoothing out the expectation term here, right?", "reply_to": "1794711330085036061", "quoted": null}, {"id": "1794719077233316115", "text": "That's a good point actually", "reply_to": "1794718003164287068", "quoted": null}, {"id": "1794718921951441129", "text": "I usually consider these as \"oh, interesting. Since that doesn't look too complicated to implement, let's bookmark this and use this in a project and see if it actually works as well as advertised. (Spoiler:  it usually doesn't.)\" With DPO itself, you find that it works pretty well but not as well as RLHF+PPO. It's good enough that more people use it than PPO at this point though -- thanks to the added convenience of not having to train a separate reward model. \nNow with SimPO, since it's super, super easy to implement, I will actually use it and see what I find. I'll probably add that to the bonus materials for Chapter 7 of my LLMs from Scratch book.\nBut all that being said, if you wait a few months, you will find follow-up papers where it turns out that the original paper was perhaps too good to be true. E.g., I saw this with DoRA the other day: ", "reply_to": null, "quoted": null}, {"id": "1794909099546796380", "text": "Come on, it's not so bad. Some ideas disappoint, but some work extremely well when implemented. See Information-Intensive (IN2) training, for example:   (TDataset:    )", "reply_to": "1794718921951441129", "quoted": null}, {"id": "1794745201657012515", "text": "Outperforming Claude 3 on AE 2 definitely won't mean our model is stronger than it in all aspects (actually it's worse on Arena-Hard than Claude 3). And overfitting probably exists b/c UltraFeedback labels are generated from GPT-4 and the judge of AE 2 is GPT-4 Turbo.", "reply_to": "1794691501488042378", "quoted": null}, {"id": "1794767043977269583", "text": "Interesting work!\n\nI'm just curious to know if the proposed modification in the loss function ensures that the aligned model is still close to the SFT model. I haven't checked the details yet :)", "reply_to": "1794055094389948546", "quoted": null}, {"id": "1794951210119643279", "text": "The policy model is not necessarily close to the SFT model. When the SFT model is not good enough, regularizing the policy against SFT may not give benefits. We find that omitting SFT regularization consistently works well for all settings we studied.", "reply_to": "1794767043977269583", "quoted": null}, {"id": "1794943603204358417", "text": "One more step towards higher quality LLMs. BAU of any LLM output should be an ongoing expert evaluation process that folds their preferred answers into the model. SPO makes this simpler.", "reply_to": null, "quoted": "1794711330085036061"}, {"id": "1795022710512230817", "text": "Seems essentially like the pairwise logistic loss (except for length normalization)   Take any binary classification loss and the difference gives you a pairwise ranking loss!", "reply_to": "1794711330085036061", "quoted": null}, {"id": "1795110655982202926", "text": "Need to look more closely but without regularization, the model may degenerate in ways that are not captured by a handful of metrics. For example, the responses may become overly long/short; capabilities other than those tested may be lost, etc.", "reply_to": "1795108731333525659", "quoted": null}, {"id": "1795111352505082364", "text": "Yeah also, how would we get the closed form, I am not very sure. Will there exist a unique solution? If it, then whats the strongly convex objective then?", "reply_to": "1795110655982202926", "quoted": null}, {"id": "1795112089473700000", "text": "Back to the KL(new model || old model): can you at least measure it and report it? In cases where KL &gt; 20, my money is on degeneration of some sorts", "reply_to": "1795111783230808352", "quoted": null}, {"id": "1795135551512719793", "text": "Good point! We'll add KLD in the next version. At least for all settings we studied, the model never degenerated (by benchmark scores + manual inspection). Of course open-ended gen eval is always hard and we're interested to see if the model degenerates in some indiscernible ways", "reply_to": "1795112089473700000", "quoted": null}, {"id": "1795129193350484151", "text": "makes sense", "reply_to": "1795112089473700000", "quoted": null}, {"id": "1795123258523574355", "text": "Nice work! We tried length normal &amp; no ref, but never together :)\n\nOne q: your motivation is that simpo aligns the train&amp;test criterion by training to simply maximize average logprob. But generally we decode with greedy decoding, which corresponds to total, not average, logprob?", "reply_to": "1794055108818325584", "quoted": null}, {"id": "1795153764229235113", "text": "Thanks\ud83d\ude00Greedy decoding should correspond to the avg prob instead of sum. It picks the token with max prob at each step so the avg prob is (approximately) maxed. The total prob is biased towards short seqs and is a bad metric for decoding (the generated seq will tend to be short)", "reply_to": "1795123258523574355", "quoted": null}, {"id": "1795148711787459002", "text": "Also I love the new margin loss being added to the formulation. Explicitly separating the classes makes a lot of sense.", "reply_to": "1795109432482812137", "quoted": null}, {"id": "1795168221835739384", "text": "If you were to personally train a model, would you be more likely to use something like this or ORPO? Or do you think they are pretty even?", "reply_to": "1794711330085036061", "quoted": null}, {"id": "1795188641918018005", "text": "Curious about: How does gamma change across different models or datasets? Additionally, without a reference policy, what does beta signify in this context?", "reply_to": "1794055094389948546", "quoted": null}, {"id": "1795192784082632815", "text": "Please refer to our Table 8 for the gamma on different datasets. We find that values between 0.5-1.5 are generally good and the performance is not very sensitive in this range. Beta is no longer the KL reg strength but a scaling of reward difference that impacts gradient weights", "reply_to": "1795188641918018005", "quoted": null}, {"id": "1795787049720480169", "text": "I was just double checking the original DPO implementation for something, and interesting enough, it looks like the original DPO authors basically already implemented the things proposed in this paper ", "reply_to": "1794711330085036061", "quoted": null}, {"id": "1814390372463980773", "text": "Nice to avoid catastrophic forgetting on fine-tuning as much as possible.", "reply_to": null, "quoted": "1814288762341396612"}, {"id": "1818722323002016072", "text": "This sounds likely to reduce generation diversity. What\u2019s your qualitative impression?", "reply_to": "1794055097212707207", "quoted": null}]