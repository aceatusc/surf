[{"id": "1743108438048272599", "text": "Why is this better than MoEs?", "reply_to": "1743094632618106981", "quoted": null}, {"id": "1743316756649652389", "text": "Bittensor and   could leap forward with the new CALM method from arXiv! It means integrating various AI models seamlessly into their network, boosting its smarts, and expanding its tasks without rebuilding from scratch. Efficiency and innovation, incentivizing for BIG WIN!", "reply_to": null, "quoted": "1743087525382537321"}, {"id": "1743403674292924620", "text": "This is the way, and its achievable.We are getting close to it day after day.", "reply_to": "1743316756649652389", "quoted": null}, {"id": "1743397126472610132", "text": "Very cool that this works! Did you run any experiments with larger models? Also, I probably missed it, how many additional parameters do you introduce in the \"glue\" layers?", "reply_to": "1743362172271603809", "quoted": null}, {"id": "1743409814557143507", "text": "This is exciting - model composition, i.e. distributed training is absolutely on the critical path.\n\nDid you benchmark against simple model merging though?", "reply_to": "1743362172271603809", "quoted": null}, {"id": "1743570160307568892", "text": "Good point! \nWe have been thinking about this as well and hope to add some comparisons soon. Given the difference in model size (b/w the models we compose), merging (afaik) is not very straightforward. There are approximation methods that we hope to try.", "reply_to": "1743409814557143507", "quoted": null}, {"id": "1743500083889369114", "text": "+1 \n\nWould be nice to benchmark against what is provided in the mergekit repo.", "reply_to": "1743409814557143507", "quoted": null}, {"id": "1743499568736776391", "text": "What are the memory implications of this?\n\nCc:   ", "reply_to": "1743362172271603809", "quoted": null}, {"id": "1743559734756597958", "text": "Looking forward to this answer\n  in 2 days", "reply_to": "1743499568736776391", "quoted": null}, {"id": "1743901380396380395", "text": "Amazing work! It would be interesting to test such an approach by combining models trained to deal with different language levels, such as text or token classification models.", "reply_to": "1743362172271603809", "quoted": null}, {"id": "1743970879774617710", "text": "Wasn't this already available in a similar form year+ ago ? i.e. same as classic ML models routing composition or similar to how semantic routing work (which could be regarded as some form of attention) in framework like   Nemo Guardrails? \n\n", "reply_to": null, "quoted": null}, {"id": "1796717706248384908", "text": "Can you elaborate in which sense LoRa is restrictive for fine-tuning?", "reply_to": "1743362172271603809", "quoted": null}]